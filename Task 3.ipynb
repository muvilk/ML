{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8cbfdf",
   "metadata": {},
   "source": [
    "# Task 3: Try other machine learning models and race to the top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a0906",
   "metadata": {},
   "source": [
    "The final model used is made of 5 other models:\n",
    "1) Logistic regression\n",
    "\n",
    "2) SVM\n",
    "\n",
    "3) Naive Bayes\n",
    "\n",
    "4) Bagging\n",
    "\n",
    "5) Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666dc56",
   "metadata": {},
   "source": [
    "Each model will make their own prediction given the data point. The final model will then make its prediction based on the majority prediction of the 5 models.\n",
    "\n",
    "Each model creates their own prediction file that can be found in task3_predictions. Alternatively, we can uncomment the code that creates the file and run the code to obtain the predictions for each model.\n",
    "\n",
    "Let's see the code implementation of all 5 models and their hyperparameter optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a001c8a5",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a3978",
   "metadata": {},
   "source": [
    "The hyperparameters to optimise is Learning rate (lr) and Batch size (bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define your train, predict, and accuracy functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "def gradient_descent(X, y, y_hat):\n",
    "    m = X.shape[0]\n",
    "    dw = (1 / m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1 / m) * np.sum(y_hat - y)\n",
    "    return dw, db\n",
    "\n",
    "def train(X, y, bs, epochs, lr, X_val, y_val):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros((n_features, 1))\n",
    "    b = 0\n",
    "    y = y.reshape(n_samples, 1)\n",
    "    \n",
    "    training_loss = []\n",
    "    validation_f1 = []\n",
    "    validation_loss = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, n_samples, bs):\n",
    "            X_batch = X[i:i + bs]\n",
    "            y_batch = y[i:i + bs]\n",
    "            y_hat = sigmoid(np.dot(X_batch, w) + b)\n",
    "            dw, db = gradient_descent(X_batch, y_batch, y_hat)\n",
    "            w -= lr * dw\n",
    "            b -= lr * db\n",
    "\n",
    "            epoch_loss += loss(y_batch, y_hat)\n",
    "        \n",
    "        epoch_loss /= (n_samples // bs)\n",
    "        training_loss.append(epoch_loss)\n",
    "\n",
    "        # Calculate validation F1 score and loss\n",
    "        y_hat_val = sigmoid(np.dot(X_val, w) + b)\n",
    "        y_pred_val = (y_hat_val > 0.5).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred_val, average='macro')\n",
    "        validation_f1.append(f1)\n",
    "        val_loss = loss(y_val, y_hat_val)\n",
    "        validation_loss.append(val_loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Training Loss: {epoch_loss:.4f}, Validation F1: {f1:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    return w, b, training_loss, validation_f1, validation_loss\n",
    "\n",
    "def predict(X, w, b):\n",
    "    y_hat = sigmoid(np.dot(X, w) + b)\n",
    "    pred = [1 if i > 0.5 else 0 for i in y_hat]\n",
    "    return np.array(pred)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def optimize_bagging(X_features, Y_label):\n",
    "    n_bags = 5  # Number of bags\n",
    "    batch_size_options = [32, 64, 128, 256]\n",
    "    learning_rate_options = [0.001, 0.01, 0.1, 0.2]\n",
    "    epochs = 500\n",
    "\n",
    "    models = []\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    indices = np.arange(X_features.shape[0])\n",
    "    all_training_losses = []\n",
    "    all_validation_f1s = []\n",
    "    all_validation_losses = []\n",
    "\n",
    "    for i in range(n_bags):\n",
    "        np.random.shuffle(indices)\n",
    "        split_idx = int(X_features.shape[0] * 0.85)\n",
    "        train_indices = indices[:split_idx]\n",
    "        val_indices = indices[split_idx:]\n",
    "\n",
    "        X_train_split = X_features[train_indices]\n",
    "        y_train_split = Y_label[train_indices]\n",
    "        X_val_split = X_features[val_indices]\n",
    "        y_val_split = Y_label[val_indices]\n",
    "\n",
    "        best_f1 = 0\n",
    "        best_params = {}\n",
    "        for lr in learning_rate_options:\n",
    "            for bs in batch_size_options:\n",
    "                w, b, training_loss, validation_f1, validation_loss = train(X_train_split, y_train_split, bs, epochs, lr, X_val_split, y_val_split)\n",
    "                y_pred = predict(X_val_split, w, b)\n",
    "                f1 = f1_score(y_val_split, y_pred, average='macro')\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_params = {'learning_rate': lr, 'batch_size': bs}\n",
    "\n",
    "                # Store training loss, validation F1 score, and validation loss for plotting\n",
    "                all_training_losses.append((training_loss, f'LR: {lr}, BS: {bs}'))\n",
    "                all_validation_f1s.append((validation_f1, f'LR: {lr}, BS: {bs}'))\n",
    "                all_validation_losses.append((validation_loss, f'LR: {lr}, BS: {bs}'))\n",
    "\n",
    "        best_lr = best_params['learning_rate']\n",
    "        best_bs = best_params['batch_size']\n",
    "        w, b, _, _, _ = train(X_train_split, y_train_split, best_bs, epochs, best_lr, X_val_split, y_val_split)\n",
    "        models.append((w, b))\n",
    "        f1_scores.append(best_f1)\n",
    "        print(f'Model {i + 1} Trained with LR: {best_lr}, BS: {best_bs}, F1 Score: {best_f1:.4f}')\n",
    "\n",
    "    # Plot training loss and validation loss for each hyperparameter combination\n",
    "    for loss, label in all_training_losses:\n",
    "        plt.plot(loss, label=f'Train {label}')\n",
    "    for loss, label in all_validation_losses:\n",
    "        plt.plot(loss, label=f'Val {label}')\n",
    "    plt.title('Training and Validation Loss for Different Hyperparameters')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot validation F1 score for each hyperparameter combination\n",
    "    for f1, label in all_validation_f1s:\n",
    "        plt.plot(f1, label=label)\n",
    "    plt.title('Validation F1 Score for Different Hyperparameters')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot F1 Score vs lr\n",
    "    for lr in learning_rate_options:\n",
    "        subset = f1_df[f1_df['Learning Rate'] == lr]\n",
    "        plt.plot(subset['Batch Size'], subset['F1 Score'], marker='o', label=f'LR: {lr}')\n",
    "    plt.title('F1 Score vs Batch Size')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot F1 Score vs batch size\n",
    "    for bs in batch_size_options:\n",
    "        subset = f1_df[f1_df['Batch Size'] == bs]\n",
    "        plt.plot(subset['Learning Rate'], subset['F1 Score'], marker='o', label=f'BS: {bs}')\n",
    "    plt.title('F1 Score vs Learning Rate')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    best_idx = np.argmax(f1_df['F1 Score'])\n",
    "    best_params = f1_df.iloc[best_idx]\n",
    "    best_lr = best_params['Learning Rate']\n",
    "    best_bs = best_params['Batch Size']\n",
    "    best_f1 = best_params['F1 Score']\n",
    "    \n",
    "    best_model_idx = np.argmax([f1 for _, _, f1 in f1_results])\n",
    "    best_model = models[best_model_idx]\n",
    "\n",
    "    print(f'Best Parameters: Learning Rate = {best_lr}, Batch Size = {best_bs}, F1 Score = {best_f1:.4f}')\n",
    "    return best_model, best_f1\n",
    "\n",
    "# Load the training data\n",
    "data = pd.read_csv(\"Data/train_tfidf_features.csv\")\n",
    "X_features = data.drop(['label', 'id'], axis=1).values\n",
    "Y_label = data['label'].values\n",
    "\n",
    "# Train the optimized bagging model\n",
    "best_model, best_f1_score = optimize_bagging(X_features, Y_label)\n",
    "best_w, best_b = best_model\n",
    "\n",
    "# Load the test data and first set of predictions\n",
    "test = pd.read_csv(\"Data/test_tfidf_features.csv\")\n",
    "\n",
    "# Predict on the test data using the best model\n",
    "X_test = test.drop(columns=['id']).values\n",
    "y_pred_best = predict(X_test, best_w, best_b)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'label': y_pred_best\n",
    "})\n",
    "#predictions_df.to_csv('Optimal_LogReg_predictions.csv', index=False) #uncomment to get file, otherwise file is found in /task3_predictions\n",
    "\n",
    "# Completed Print\n",
    "#print(\"Final predictions saved to Optimal_LogReg_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0acd4",
   "metadata": {},
   "source": [
    "The optimal hyperparater values are\n",
    "Learning rate: 0.1\n",
    "Batch sizeL: 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30edd0",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0fec0",
   "metadata": {},
   "source": [
    "SVM with different kernel functions were investigated:\n",
    "- Linear SVM\n",
    "- Radial Basis function SVM*\n",
    "- Sigmoid SVM\n",
    "- Poly SVM\n",
    "\n",
    "Each of them were evaluated by splitting the training data 80-20, obtaining a training and validation set in order to calculate thier macro F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e9bbb",
   "metadata": {},
   "source": [
    "#### Linear SVM\n",
    "\n",
    "Hyperparameters:\n",
    "C: regularisation term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21340d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "X = data.drop(columns=['id', 'label']).values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PLAIN AND SIMPLE LINEAR SVM USING SVC KERNEL = LINEAR\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('svc', SVC(kernel='linear',\n",
    "               gamma=0.1))\n",
    "])\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10, 50, 100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro') \n",
    "\n",
    "\n",
    "print(f'Accuracy with SGDClassifier: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report with SGDClassifier:')\n",
    "print(report)\n",
    "\n",
    "# HISTORY\n",
    "# first run: 0.7247599650858306, 0.6926448445963982 (6min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a5835",
   "metadata": {},
   "source": [
    "Optimising linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR SVM WITH DIFFERING REGULARISATION VALUE USING LINEARSVC\n",
    "# https://scikit-learn.org/stable/auto_examples/svm/plot_svm_scale_c.html#sphx-glr-auto-examples-svm-plot-svm-scale-c-py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('svc', LinearSVC(penalty=\"l1\", loss=\"squared_hinge\", dual=False, tol=1e-3))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svc__C': [0.17385, 0.173875, 0.1739, 0.173925, 0.17395],  # Even finer range around 0.1739\n",
    "    'svc__max_iter': [6000, 6200, 6300, 6400, 6500],\n",
    "    'svc__loss': ['hinge','squared_hinge']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=7, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f'Best parameters: {best_params}')\n",
    "\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\"\"\" Best parameters: {'svc__C': 0.173875, 'svc__loss': 'squared_hinge', 'svc__max_iter': 6000}\n",
    "Accuracy: 0.722432353796916\n",
    "F1 Score: 0.6880534155931336 with cv=5. 4 mins\"\"\"\n",
    "\n",
    "\"\"\" Best parameters: {'svc__C': 0.17385, 'svc__loss': 'squared_hinge', 'svc__max_iter': 6000}\n",
    "Accuracy: 0.722432353796916\n",
    "F1 Score: 0.6880534155931336 with cv=7. 4 mins\"\"\"\n",
    "\n",
    "# PLAIN AND SIMPLE LINEAR SVM USING SVC KERNEL = LINEAR WITH PCA\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score,f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "X = data.drop(columns=['id', 'label']).values\n",
    "y = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pca = PCA(n_components=700)  \n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.fit_transform(X_test)\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "svm_model.fit(X_train_pca, y_train)\n",
    "y_pred = svm_model.predict(X_test_pca)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "\n",
    "# HISTORY\n",
    "# first run: 0.5609 (1min 500 components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d51eb",
   "metadata": {},
   "source": [
    "#### Sigmoid SVM\n",
    "\n",
    "Hyperparameters:\n",
    "- C: regularisation term\n",
    "- coef: independent term added to the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdaa2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid SVM\n",
    "# https://stats.stackexchange.com/questions/90736/the-difference-of-kernels-in-svm\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score,f1_score\n",
    "\n",
    "\n",
    "svm_sigmoid = SVC(kernel='sigmoid', C=1.0, coef0=1)\n",
    "svm_sigmoid.fit(X_train, y_train)\n",
    "y_pred = svm_sigmoid.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "'''\n",
    "Accuracy: 0.5292406168169915\n",
    "F1 Score: 0.49756365081946474\n",
    "Noting the poor performance, sigmoid SVM was not investigated further\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b532fce6",
   "metadata": {},
   "source": [
    "#### Poly SVM\n",
    "\n",
    "Hyperparameters:\n",
    "- C: regularisation term\n",
    "- coef: independent term added to the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a346adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLY SVM\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "svm_poly = SVC(kernel='poly', degree=5, C=1.0, coef0=1) # here we will explore different degrees\n",
    "\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "'''\n",
    "Accuracy: 0.6264183881291824\n",
    "F1 Score: 0.3851520572450805\n",
    "Noting the poor performance, Poly SVM was not investigated further\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf2cffd",
   "metadata": {},
   "source": [
    "#### Radial basis function SVM \n",
    "\n",
    "This was chosen as the final model for SVM.\n",
    "\n",
    "Hyperparamters:\n",
    "- k: feature selection size\n",
    "- c: regularisation term \n",
    "- gamma: kernel coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radial Basis function SVM\n",
    "# https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score,f1_score\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectKBest(chi2)), \n",
    "    ('rbf', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'feature_selection__k': [3000, 4000, 5000],\n",
    "    'rbf__C': [ 5, 10, 20,30, 50],\n",
    "    'rbf__gamma': ['scale', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1_macro', n_jobs=-1) \n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "print(f'Best parameters: {best_params}')\n",
    "# Evaluate the best model on the test set\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36be52",
   "metadata": {},
   "source": [
    "Training the SVM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2be069",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('data/test_tfidf_features.csv')\n",
    "x_test_dataset = test_dataset.drop(columns=['id']).values\n",
    "y_test_pred = best_model.predict(x_test_dataset)\n",
    "predictions_df = pd.DataFrame({'id': test_dataset['id'], 'label': y_test_pred})\n",
    "# Save the DataFrame to a CSV file\n",
    "#predictions_df.to_csv('Optimal_svm.csv', index=False)  #uncomment to get file, otherwise file is found in /task3_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240dc61f",
   "metadata": {},
   "source": [
    "validation of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a991c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "\"\"\" # feature selection\n",
    "k = 500 \n",
    "chi2_features = SelectKBest(chi2, k=k) \"\"\"\n",
    "\n",
    "svm = SVC()\n",
    "\"\"\" ('feature_selection', chi2_features), \"\"\"\n",
    "pipeline = Pipeline([\n",
    "\n",
    "    ('classification', svm)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classification__C': np.logspace(-3, 3, 7),\n",
    "    'classification__kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'classification__gamma': ['scale', 'auto']\n",
    "\n",
    "}\n",
    "\n",
    "#cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f'Best parameters: {best_params}')\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "'''\n",
    "Best parameters: {'classification__C': 10.0, 'classification__gamma': 'scale', 'classification__kernel': 'rbf'}\n",
    "Accuracy: 0.7294151876636602\n",
    "F1 Score: 0.69934251368587\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab26695",
   "metadata": {},
   "source": [
    "Optimal hyperparameters:\n",
    "- k: 5000\n",
    "- C: 5\n",
    "- gamma: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106615ea",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb848a",
   "metadata": {},
   "source": [
    "The hyperparameter optimised is alpha, also known as smoothing factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, make_scorer\n",
    "import joblib\n",
    "\n",
    "# Loading the training data\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "X = data.drop(columns=['id', 'label'])\n",
    "Y = data['label']\n",
    "\n",
    "\n",
    "'''\n",
    "MultinomialNB has 2 main parameters (alpha and fit_prior)\n",
    "alpha is the smoothing parameter, preventing the model from assigning 0 probability to unseen words\n",
    "adds alpha to count when calculating conditional probability. (default =1)\n",
    "fit_prior is a boolean that sets whether the model will calculate prior probabilities from the training set. (default = True)\n",
    "Also important to note that SciKit uses log likelihood when calculating \n",
    "'''\n",
    "#Set Up a Detailed Hyperparameter Grid for Alpha\n",
    "alpha_values = np.arange(0.01, 1, 0.01)  # More detailed range of alpha values\n",
    "\n",
    "#Perform K-Fold Cross-Validation for Each Alpha\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "results = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    model = MultinomialNB(alpha=alpha)\n",
    "    cv_scores = cross_val_score(model, X_train, Y_train, cv=kf, scoring=scorer)\n",
    "    results.append((alpha, np.mean(cv_scores)))\n",
    "    print(f\"Alpha: {alpha}, F1 Score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Convert results to a DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(results, columns=['Alpha', 'F1 Score'])\n",
    "\n",
    "#Plot the Results to Find the Optimal Alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['Alpha'], results_df['F1 Score'], marker='o')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('F1 Score (Macro)')\n",
    "plt.title('Alpha vs. F1 Score (Macro)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Find the Exact Optimal Alpha Value\n",
    "optimal_alpha = results_df.loc[results_df['F1 Score'].idxmax()]['Alpha']\n",
    "optimal_f1_score = results_df.loc[results_df['F1 Score'].idxmax()]['F1 Score']\n",
    "\n",
    "print(f\"Optimal alpha value: {optimal_alpha}\")\n",
    "print(f\"F1 Score at optimal alpha: {optimal_f1_score}\")\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = MultinomialNB(alpha = 0.26)\n",
    "\n",
    "# Load the test data\n",
    "test = pd.read_csv(\"Data/test_tfidf_features.csv\")\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "\n",
    "# Fit the model on the entire training data\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Create a DataFrame with predictions\n",
    "predictions_df = pd.DataFrame({'id': test['id'], 'label': y_test_pred})\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "#predictions_df.to_csv('Optimal_NB.csv', index=False) #uncomment to get file, otherwise file is found in /task3_predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039995b",
   "metadata": {},
   "source": [
    "The optimal alpha value is alpha = 0.26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81987095",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23d783",
   "metadata": {},
   "source": [
    "Bagging was investigated using SKlearn's random forest classifier.\n",
    "\n",
    "The hyperparameters to optimise were \n",
    "- n_estimators: The number of trees in the forest.\n",
    "- Criterion: The function to measure the quality of a split.\n",
    "- max_depth: The maximum depth of the tree.  \n",
    "- min_samples_split: The minimum number of samples required to split an internal node\n",
    "- min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "- max_features: The number of features to consider when looking for the best split\n",
    "- max_samples: The number of samples to draw to train each tree.\n",
    "- class_weight: Weights associated with classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "df = pd.read_csv('data/train_tfidf_features.csv')\n",
    "k = 5\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=['id', 'label'])\n",
    "y = df['label']\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=400, random_state=42, criterion=\"entropy\", min_samples_split=80, max_features='sqrt', max_samples=0.6, \n",
    "                               class_weight='balanced_subsample', max_depth=150, min_samples_leaf=2)\n",
    "\n",
    "if False:  # Change this to True to activate hyperparameter optimization\n",
    "    param_grid = {\n",
    "        'n_estimators': [350,360,370,380,390]\n",
    "    }\n",
    "    \n",
    "    print(\"Doing hyperparameter optimization\")\n",
    "    halving_grid_search = HalvingGridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        factor=3,  # The factor by which the number of candidates is reduced at each iteration\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    halving_grid_search.fit(X_train, y_train)\n",
    "    print(\"Best parameters found: \", halving_grid_search.best_params_)\n",
    "\n",
    "    # Collecting results\n",
    "    results = pd.DataFrame(halving_grid_search.cv_results_)\n",
    "\n",
    "    # Best model evaluation\n",
    "    best_model = halving_grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "if True:\n",
    "    print(\"Doing Cross Validation\")\n",
    "    scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "    print(f\"Cross-Validation F1 Scores for {k} folds: {scores}\")\n",
    "    print(\"Mean Accuracy:\", scores.mean())\n",
    "    print(\"Standard Deviation:\", scores.std())\n",
    "\n",
    "if True:\n",
    "    test = pd.read_csv(\"data/test_tfidf_features.csv\")\n",
    "    X_test = test.drop(['id'], axis=1).values\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions_df = pd.DataFrame({'id': test['id'], 'label': y_pred})\n",
    "\n",
    "    # Display the first few rows of the predictions\n",
    "    print(predictions_df.head())\n",
    "\n",
    "    # Save predictions to a CSV file\n",
    "    #predictions_df.to_csv('Optimal_bagging', index=False) #uncomment to get file, otherwise file is found in /task3_predictions\n",
    "\n",
    "\n",
    "if False: #creating graphs for each hyperparameter\n",
    "    min_samples_split_range = range(2, 100, 1)\n",
    "\n",
    "    # Store the mean and standard deviation of the F1 Macro scores for each n_estimators\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for min_samples_split in min_samples_split_range:\n",
    "        print(f\"Evaluating model with min_samples_split={min_samples_split}\")\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, criterion=\"entropy\",\n",
    "                                       min_samples_split=min_samples_split, max_features='sqrt', max_samples=0.7,\n",
    "                                       class_weight='balanced_subsample', max_depth=80, min_samples_leaf=1)\n",
    "        scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "        mean_scores.append(scores.mean())\n",
    "        std_scores.append(scores.std())\n",
    "        print(f\"Mean F1 Macro Score: {scores.mean()} | Std Dev: {scores.std()}\")\n",
    "\n",
    "    # Save the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(min_samples_split_range, mean_scores, yerr=std_scores, fmt='-o')\n",
    "    plt.xlabel('min_samples_split')\n",
    "    plt.ylabel('Mean F1 Macro Score')\n",
    "    plt.title('F1 Macro Score vs. min_samples_split')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('min_samples_split_vs_f1_macro.png')\n",
    "    plt.close()\n",
    "\n",
    "    max_depth_range = range(1, 200, 1)\n",
    "\n",
    "    # Store the mean and standard deviation of the F1 Macro scores for each n_estimators\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for max_depth in max_depth_range:\n",
    "        print(f\"Evaluating model with max_depth={max_depth}\")\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, criterion=\"entropy\",\n",
    "                                       min_samples_split=7, max_features='sqrt', max_samples=0.7,\n",
    "                                       class_weight='balanced_subsample', max_depth=max_depth, min_samples_leaf=1)\n",
    "        scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "        mean_scores.append(scores.mean())\n",
    "        std_scores.append(scores.std())\n",
    "        print(f\"Mean F1 Macro Score: {scores.mean()} | Std Dev: {scores.std()}\")\n",
    "\n",
    "    # Save the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(max_depth_range, mean_scores, yerr=std_scores, fmt='-o')\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('Mean F1 Macro Score')\n",
    "    plt.title('F1 Macro Score vs. max_depth')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('max_depth_vs_f1_macro.png')\n",
    "    plt.close()\n",
    "\n",
    "    min_samples_leaf_range = range(1, 100, 1)\n",
    "\n",
    "    # Store the mean and standard deviation of the F1 Macro scores for each n_estimators\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for min_samples_leaf in min_samples_leaf_range:\n",
    "        print(f\"Evaluating model with min_samples_leaf={min_samples_leaf}\")\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, criterion=\"entropy\",\n",
    "                                       min_samples_split=7, max_features='sqrt', max_samples=0.7,\n",
    "                                       class_weight='balanced_subsample', max_depth=80, min_samples_leaf=min_samples_leaf)\n",
    "        scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "        mean_scores.append(scores.mean())\n",
    "        std_scores.append(scores.std())\n",
    "        print(f\"Mean F1 Macro Score: {scores.mean()} | Std Dev: {scores.std()}\")\n",
    "\n",
    "    # Save the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(min_samples_leaf_range, mean_scores, yerr=std_scores, fmt='-o')\n",
    "    plt.xlabel('min_samples_leaf')\n",
    "    plt.ylabel('Mean F1 Macro Score')\n",
    "    plt.title('F1 Macro Score vs. min_samples_leaf')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('min_samples_leaf_vs_f1_macro.png')\n",
    "    plt.close()\n",
    "\n",
    "    max_samples_range = np.arange(0.01, 1, 0.01)\n",
    "\n",
    "    # Store the mean and standard deviation of the F1 Macro scores for each n_estimators\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for max_samples in max_samples_range:\n",
    "        print(f\"Evaluating model with max_samples={max_samples}\")\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, criterion=\"entropy\",\n",
    "                                       min_samples_split=7, max_features='sqrt', max_samples=max_samples,\n",
    "                                       class_weight='balanced_subsample', max_depth=80, min_samples_leaf=1)\n",
    "        scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "        mean_scores.append(scores.mean())\n",
    "        std_scores.append(scores.std())\n",
    "        print(f\"Mean F1 Macro Score: {scores.mean()} | Std Dev: {scores.std()}\")\n",
    "\n",
    "    # Save the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(max_samples_range, mean_scores, yerr=std_scores, fmt='-o')\n",
    "    plt.xlabel('max_samples')\n",
    "    plt.ylabel('Mean F1 Macro Score')\n",
    "    plt.title('F1 Macro Score vs. max_samples')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('max_samples_vs_f1_macro.png')\n",
    "    plt.close()\n",
    "\n",
    "    n_estimators_range = range(10, 510, 10)\n",
    "\n",
    "    # Store the mean and standard deviation of the F1 Macro scores for each n_estimators\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for n_estimators in n_estimators_range:\n",
    "        print(f\"Evaluating model with n_estimators={n_estimators}\")\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, random_state=42, criterion=\"entropy\",\n",
    "                                       min_samples_split=7, max_features='sqrt', max_samples=0.7,\n",
    "                                       class_weight='balanced_subsample', max_depth=80, min_samples_leaf=1)\n",
    "        scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "        mean_scores.append(scores.mean())\n",
    "        std_scores.append(scores.std())\n",
    "        print(f\"Mean F1 Macro Score: {scores.mean()} | Std Dev: {scores.std()}\")\n",
    "\n",
    "    # Save the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(n_estimators_range, mean_scores, yerr=std_scores, fmt='-o')\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Mean F1 Macro Score')\n",
    "    plt.title('F1 Macro Score vs. n_estimators')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('n_estimators_vs_f1_macro.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cc195",
   "metadata": {},
   "source": [
    "The optimal values for each hyperparameters are:\n",
    "- n_estimators: 400\n",
    "- Criterion: \"entropy\" \n",
    "- max_depth: 150  \n",
    "- min_samples_split: 80\n",
    "- min_samples_leaf: 2\n",
    "- max_features: 'sqrt'\n",
    "- max_samples: 0.6\n",
    "- class_weight: 'balanced_subsample'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e4dd95",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630a44c",
   "metadata": {},
   "source": [
    "Boosting was investigated using Light Gradient Boosting Machine (LightGBM)\n",
    "\n",
    "The hyperparameters to optimise were\n",
    "- num_leaves: The maximum number of leaves in one tree\n",
    "- n_estimators: The number of boosting iterations (trees).\n",
    "- max_depth: The maximum depth of each tree.\n",
    "- min_child_samples: The minimum number of data points needed in a leaf.\n",
    "- colsample_bytree: The fraction of features to consider when building each tree.\n",
    "- scale_pos_weight: The weight for balancing the positive and negative classes in the binary classification task.\n",
    "- reg_alpha:  L1 regularization term on weights\n",
    "- reg_lambda: L2 regularization term on weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac22dc",
   "metadata": {},
   "source": [
    "Deciding between models:\n",
    "- Decision Stump\n",
    "- Logistic Regression\n",
    "- Linear SVM\n",
    "- Gaussian Naive Bayes\n",
    "- GBDT\n",
    "- DART\n",
    "- GOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3977fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values  # Convert to NumPy array\n",
    "y = data['label'].values  # Convert to NumPy array\n",
    "\n",
    "# List of random seeds\n",
    "random_seeds = [42, 52, 62]\n",
    "\n",
    "# List of boosting algorithms and weak learners\n",
    "boosting_algorithms = {\n",
    "    'LightGBM GBDT': LGBMClassifier(boosting_type='gbdt'),\n",
    "    'LightGBM DART': LGBMClassifier(boosting_type='dart'),\n",
    "    'LightGBM GOSS': LGBMClassifier(boosting_type='goss')\n",
    "}\n",
    "\n",
    "weak_learners = {\n",
    "    'Decision Stump': DecisionTreeClassifier(max_depth=1),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Gaussian Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store F1 scores and times for each algorithm/learner\n",
    "boosting_results = {name: {'f1_scores': [], 'times': []} for name in boosting_algorithms}\n",
    "weak_learner_results = {name: {'f1_scores': [], 'times': []} for name in weak_learners}\n",
    "\n",
    "def evaluate_boosting(name, model, seed):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Initialize the model\n",
    "    model.set_params(random_state=seed)\n",
    "    \n",
    "    # Measure the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Measure the end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the time taken\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the macro F1 score on the test set\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    # Return the F1 score and time taken\n",
    "    return name, seed, f1, elapsed_time\n",
    "\n",
    "def evaluate_learner(name, base_estimator, seed):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Initialize the AdaBoost classifier\n",
    "    adaboost = AdaBoostClassifier(estimator=base_estimator, n_estimators=10, learning_rate=1.0, random_state=seed)\n",
    "    \n",
    "    # Measure the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the classifier\n",
    "    adaboost.fit(X_train, y_train)\n",
    "    \n",
    "    # Measure the end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the time taken\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = adaboost.predict(X_test)\n",
    "    \n",
    "    # Calculate the macro F1 score on the test set\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    # Return the F1 score and time taken\n",
    "    return name, seed, f1, elapsed_time\n",
    "\n",
    "# Use Parallel and delayed to parallelize the evaluation\n",
    "results_boosting = Parallel(n_jobs=-1)(delayed(evaluate_boosting)(name, model, seed)\n",
    "                                       for name, model in boosting_algorithms.items()\n",
    "                                       for seed in random_seeds)\n",
    "\n",
    "results_weak_learners = Parallel(n_jobs=-1)(delayed(evaluate_learner)(name, base_estimator, seed)\n",
    "                                            for name, base_estimator in weak_learners.items()\n",
    "                                            for seed in random_seeds)\n",
    "\n",
    "# Process results for boosting algorithms\n",
    "for name, seed, f1, elapsed_time in results_boosting:\n",
    "    boosting_results[name]['f1_scores'].append(f1)\n",
    "    boosting_results[name]['times'].append(elapsed_time)\n",
    "\n",
    "# Process results for weak learners\n",
    "for name, seed, f1, elapsed_time in results_weak_learners:\n",
    "    weak_learner_results[name]['f1_scores'].append(f1)\n",
    "    weak_learner_results[name]['times'].append(elapsed_time)\n",
    "\n",
    "# Calculate the mean and standard deviation of F1 scores and times for each algorithm/learner\n",
    "boosting_means_f1 = {name: np.mean(boosting_results[name]['f1_scores']) for name in boosting_algorithms}\n",
    "boosting_stds_f1 = {name: np.std(boosting_results[name]['f1_scores']) for name in boosting_algorithms}\n",
    "boosting_means_time = {name: np.mean(boosting_results[name]['times']) for name in boosting_algorithms}\n",
    "boosting_stds_time = {name: np.std(boosting_results[name]['times']) for name in boosting_algorithms}\n",
    "\n",
    "learner_means_f1 = {name: np.mean(weak_learner_results[name]['f1_scores']) for name in weak_learners}\n",
    "learner_stds_f1 = {name: np.std(weak_learner_results[name]['f1_scores']) for name in weak_learners}\n",
    "learner_means_time = {name: np.mean(weak_learner_results[name]['times']) for name in weak_learners}\n",
    "learner_stds_time = {name: np.std(weak_learner_results[name]['times']) for name in weak_learners}\n",
    "\n",
    "# Combine the results for plotting\n",
    "all_names = list(boosting_means_f1.keys()) + list(learner_means_f1.keys())\n",
    "all_means_f1 = list(boosting_means_f1.values()) + list(learner_means_f1.values())\n",
    "all_stds_f1 = list(boosting_stds_f1.values()) + list(learner_stds_f1.values())\n",
    "all_means_time = list(boosting_means_time.values()) + list(learner_means_time.values())\n",
    "all_stds_time = list(boosting_stds_time.values()) + list(learner_stds_time.values())\n",
    "\n",
    "# Create a dual dot plot\n",
    "fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot F1 scores\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Algorithm / Weak Learner')\n",
    "ax1.set_ylabel('Mean Macro F1 Score', color=color)\n",
    "ax1.errorbar(all_names, all_means_f1, yerr=all_stds_f1, fmt='o', capsize=5, color=color, label='Mean F1 Score')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a second y-axis to plot times\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Mean Time Taken (seconds)', color=color)\n",
    "ax2.errorbar(all_names, all_means_time, yerr=all_stds_time, fmt='o', capsize=5, color=color, label='Mean Time Taken')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Add a title and grid\n",
    "plt.title('Mean Macro F1 Score and Mean Time Taken vs. Algorithm with Error Bars')\n",
    "ax1.grid(True)\n",
    "fig.tight_layout()\n",
    "plt.xticks(rotation=45)\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Print the mean and standard deviation of F1 scores and times for each algorithm/learner\n",
    "for name in all_names:\n",
    "    if name in boosting_means_f1:\n",
    "        print(f\"Boosting Algorithm: {name}, Mean F1 Score: {boosting_means_f1[name]:.2f}, Std: {boosting_stds_f1[name]:.2f}\")\n",
    "        print(f\"Boosting Algorithm: {name}, Mean Time Taken: {boosting_means_time[name]:.2f} seconds, Std: {boosting_stds_time[name]:.2f}\")\n",
    "    else:\n",
    "        print(f\"Weak Learner: {name}, Mean F1 Score: {learner_means_f1[name]:.2f}, Std: {learner_stds_f1[name]:.2f}\")\n",
    "        print(f\"Weak Learner: {name}, Mean Time Taken: {learner_means_time[name]:.2f} seconds, Std: {learner_stds_time[name]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63ccb8",
   "metadata": {},
   "source": [
    "GBDT was selected as the model to undergo further optimisation of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209b677",
   "metadata": {},
   "source": [
    "#### Optimisation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f072f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# List of hyperparameters and their ranges\n",
    "hyperparameters = {\n",
    "    'num_leaves': [15, 31, 63, 127, 255],\n",
    "    'n_estimators': [50, 100, 200, 400, 800],\n",
    "    'max_depth': [-1, 3, 5, 7, 9, 12],\n",
    "    'min_child_samples': [10, 20, 50, 100, 150],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'scale_pos_weight': [1, 2, 5, 10, 20],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1, 2],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1, 2]\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {param: {'train_f1_scores': [], 'test_f1_scores': []} for param in hyperparameters}\n",
    "\n",
    "def evaluate_hyperparameter(param_name, param_values):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"Evaluating {param_name} with value {value}...\")\n",
    "        train_f1_scores = []\n",
    "        test_f1_scores = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 31,  # Default\n",
    "                'n_estimators': 100,  # Default\n",
    "                'max_depth': -1,  # Default\n",
    "                'min_child_samples': 20,  # Default\n",
    "                'subsample': 1.0,  # Default\n",
    "                'colsample_bytree': 1.0,  # Default\n",
    "                'scale_pos_weight': 1,  # Default\n",
    "                'reg_alpha': 0,  # Default\n",
    "                'reg_lambda': 0,  # Default\n",
    "                'random_state': 123\n",
    "            }\n",
    "            params[param_name] = value\n",
    "            \n",
    "            model = LGBMClassifier(**params)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_val_pred = model.predict(X_val_fold)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                train_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "                test_f1_scores.append(f1_score(y_test, y_test_pred, average='macro'))\n",
    "            except Exception as e:\n",
    "                print(f\"Error while training or predicting with {param_name}={value}: {e}\")\n",
    "\n",
    "        if train_f1_scores and test_f1_scores:\n",
    "            results[param_name]['train_f1_scores'].append(np.mean(train_f1_scores))\n",
    "            results[param_name]['test_f1_scores'].append(np.mean(test_f1_scores))\n",
    "        else:\n",
    "            print(f\"No scores collected for {param_name} with value {value}\")\n",
    "\n",
    "# Evaluate each hyperparameter\n",
    "for param_name, param_values in hyperparameters.items():\n",
    "    evaluate_hyperparameter(param_name, param_values)\n",
    "\n",
    "# Plotting results for each hyperparameter\n",
    "for param_name in hyperparameters:\n",
    "    if results[param_name]['train_f1_scores']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['train_f1_scores'], marker='o', label='Train F1 Score')\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['test_f1_scores'], marker='o', label='Test F1 Score')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Macro F1 Score vs. {param_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No results for hyperparameter {param_name}\")\n",
    "\n",
    "# Print results\n",
    "for param_name in hyperparameters:\n",
    "    print(f\"Hyperparameter: {param_name}\")\n",
    "    print(f\"Values: {hyperparameters[param_name]}\")\n",
    "    print(f\"Train F1 Scores: {results[param_name]['train_f1_scores']}\")\n",
    "    print(f\"Test F1 Scores: {results[param_name]['test_f1_scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8f79a",
   "metadata": {},
   "source": [
    "#### Optimisation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# List of hyperparameters and their ranges\n",
    "hyperparameters = {\n",
    "    'num_leaves': [31, 40, 50, 63, 70, 80, 90, 100],\n",
    "    'n_estimators': [75, 100, 125, 150, 175, 200],\n",
    "    'max_depth': [-1, 10, 20, 30, 40, 50, 60, 70],\n",
    "    'min_child_samples': [1, 4, 8, 12, 16, 20],\n",
    "    'scale_pos_weight': [1, 1.25, 1.5, 1.75, 2, 2.25, 2.5],\n",
    "    'reg_alpha': [0, 0.4, 0.8, 1.2, 1.6, 2],\n",
    "    'reg_lambda': [0, 0.4, 0.8, 1.2, 1.6, 2]\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {param: {'train_f1_scores': [], 'test_f1_scores': []} for param in hyperparameters}\n",
    "\n",
    "def evaluate_hyperparameter(param_name, param_values):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"Evaluating {param_name} with value {value}...\")\n",
    "        train_f1_scores = []\n",
    "        test_f1_scores = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 63,  # Default\n",
    "                'n_estimators': 100,  # Default\n",
    "                'max_depth': -1,  # Default\n",
    "                'min_child_samples': 10,  # Default\n",
    "                'subsample': 1.0,  # Default\n",
    "                'colsample_bytree': 1.0,  # Default\n",
    "                'scale_pos_weight': 1,  # Default\n",
    "                'reg_alpha': 0,  # Default\n",
    "                'reg_lambda': 0,  # Default\n",
    "                'random_state': 123\n",
    "            }\n",
    "            params[param_name] = value\n",
    "            \n",
    "            model = LGBMClassifier(**params)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_val_pred = model.predict(X_val_fold)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                train_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "                test_f1_scores.append(f1_score(y_test, y_test_pred, average='macro'))\n",
    "            except Exception as e:\n",
    "                print(f\"Error while training or predicting with {param_name}={value}: {e}\")\n",
    "\n",
    "        if train_f1_scores and test_f1_scores:\n",
    "            results[param_name]['train_f1_scores'].append(np.mean(train_f1_scores))\n",
    "            results[param_name]['test_f1_scores'].append(np.mean(test_f1_scores))\n",
    "        else:\n",
    "            print(f\"No scores collected for {param_name} with value {value}\")\n",
    "\n",
    "# Evaluate each hyperparameter\n",
    "for param_name, param_values in hyperparameters.items():\n",
    "    evaluate_hyperparameter(param_name, param_values)\n",
    "\n",
    "# Plotting results for each hyperparameter\n",
    "for param_name in hyperparameters:\n",
    "    if results[param_name]['train_f1_scores']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['train_f1_scores'], marker='o', label='Train F1 Score')\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['test_f1_scores'], marker='o', label='Test F1 Score')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Macro F1 Score vs. {param_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No results for hyperparameter {param_name}\")\n",
    "\n",
    "# Print results\n",
    "for param_name in hyperparameters:\n",
    "    print(f\"Hyperparameter: {param_name}\")\n",
    "    print(f\"Values: {hyperparameters[param_name]}\")\n",
    "    print(f\"Train F1 Scores: {results[param_name]['train_f1_scores']}\")\n",
    "    print(f\"Test F1 Scores: {results[param_name]['test_f1_scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e284985e",
   "metadata": {},
   "source": [
    "#### Optimisation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81bde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# List of hyperparameters and their ranges\n",
    "hyperparameters = {\n",
    "    'num_leaves': [31, 40, 50, 63, 70, 80, 90, 100, 110, 120, 130],\n",
    "    'n_estimators': [75, 85, 95, 105, 115, 125],\n",
    "    'max_depth': [-1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'min_child_samples': [1, 2, 4, 6, 8, 10],\n",
    "    'reg_lambda': [1, 1.2, 1.4, 1.6, 1.8, 2.0]\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {param: {'train_f1_scores': [], 'test_f1_scores': []} for param in hyperparameters}\n",
    "\n",
    "def evaluate_hyperparameter(param_name, param_values):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"Evaluating {param_name} with value {value}...\")\n",
    "        train_f1_scores = []\n",
    "        test_f1_scores = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 63,  # Default\n",
    "                'n_estimators': 100,  # Default\n",
    "                'max_depth': -1,  # Default\n",
    "                'min_child_samples': 10,  # Default\n",
    "                'subsample': 1.0,  # Default\n",
    "                'colsample_bytree': 1.0,  # Default\n",
    "                'scale_pos_weight': 1.5,  # DONE\n",
    "                'reg_alpha': 1.2,  # DONE\n",
    "                'reg_lambda': 0,  # Default\n",
    "                'random_state': 123\n",
    "            }\n",
    "            params[param_name] = value\n",
    "            \n",
    "            model = LGBMClassifier(**params)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_val_pred = model.predict(X_val_fold)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                train_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "                test_f1_scores.append(f1_score(y_test, y_test_pred, average='macro'))\n",
    "            except Exception as e:\n",
    "                print(f\"Error while training or predicting with {param_name}={value}: {e}\")\n",
    "\n",
    "        if train_f1_scores and test_f1_scores:\n",
    "            results[param_name]['train_f1_scores'].append(np.mean(train_f1_scores))\n",
    "            results[param_name]['test_f1_scores'].append(np.mean(test_f1_scores))\n",
    "        else:\n",
    "            print(f\"No scores collected for {param_name} with value {value}\")\n",
    "\n",
    "# Evaluate each hyperparameter\n",
    "for param_name, param_values in hyperparameters.items():\n",
    "    evaluate_hyperparameter(param_name, param_values)\n",
    "\n",
    "# Plotting results for each hyperparameter\n",
    "for param_name in hyperparameters:\n",
    "    if results[param_name]['train_f1_scores']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['train_f1_scores'], marker='o', label='Train F1 Score')\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['test_f1_scores'], marker='o', label='Test F1 Score')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Macro F1 Score vs. {param_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No results for hyperparameter {param_name}\")\n",
    "\n",
    "# Print results\n",
    "for param_name in hyperparameters:\n",
    "    print(f\"Hyperparameter: {param_name}\")\n",
    "    print(f\"Values: {hyperparameters[param_name]}\")\n",
    "    print(f\"Train F1 Scores: {results[param_name]['train_f1_scores']}\")\n",
    "    print(f\"Test F1 Scores: {results[param_name]['test_f1_scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b7446",
   "metadata": {},
   "source": [
    "#### Optimisation 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e910ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# List of hyperparameters and their ranges\n",
    "hyperparameters = {\n",
    "    'min_child_samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'colsample_bytree': [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1],\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {param: {'train_f1_scores': [], 'test_f1_scores': []} for param in hyperparameters}\n",
    "\n",
    "def evaluate_hyperparameter(param_name, param_values):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"Evaluating {param_name} with value {value}...\")\n",
    "        train_f1_scores = []\n",
    "        test_f1_scores = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 70,\n",
    "                'n_estimators': 85,\n",
    "                'max_depth': 30,\n",
    "                'min_child_samples': 2,\n",
    "                'subsample': 1.0,  # Default\n",
    "                'colsample_bytree': 1.0,  # Default\n",
    "                'scale_pos_weight': 1.5,\n",
    "                'reg_alpha': 1.2,\n",
    "                'reg_lambda': 1,\n",
    "                'random_state': 123\n",
    "            }\n",
    "            params[param_name] = value\n",
    "            \n",
    "            model = LGBMClassifier(**params)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_val_pred = model.predict(X_val_fold)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                train_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "                test_f1_scores.append(f1_score(y_test, y_test_pred, average='macro'))\n",
    "                \n",
    "                # Check the impact on training\n",
    "                print(f\"Trained with {param_name}={value} in {time.time() - start_time:.2f} seconds\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while training or predicting with {param_name}={value}: {e}\")\n",
    "\n",
    "        if train_f1_scores and test_f1_scores:\n",
    "            results[param_name]['train_f1_scores'].append(np.mean(train_f1_scores))\n",
    "            results[param_name]['test_f1_scores'].append(np.mean(test_f1_scores))\n",
    "        else:\n",
    "            print(f\"No scores collected for {param_name} with value {value}\")\n",
    "\n",
    "# Evaluate each hyperparameter\n",
    "for param_name, param_values in hyperparameters.items():\n",
    "    evaluate_hyperparameter(param_name, param_values)\n",
    "\n",
    "# Plotting results for each hyperparameter\n",
    "for param_name in hyperparameters:\n",
    "    if results[param_name]['train_f1_scores']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['train_f1_scores'], marker='o', label='Train F1 Score')\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['test_f1_scores'], marker='o', label='Test F1 Score')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Macro F1 Score vs. {param_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No results for hyperparameter {param_name}\")\n",
    "\n",
    "# Print results\n",
    "for param_name in hyperparameters:\n",
    "    print(f\"Hyperparameter: {param_name}\")\n",
    "    print(f\"Values: {hyperparameters[param_name]}\")\n",
    "    print(f\"Train F1 Scores: {results[param_name]['train_f1_scores']}\")\n",
    "    print(f\"Test F1 Scores: {results[param_name]['test_f1_scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cd1b3",
   "metadata": {},
   "source": [
    "#### Optimisation 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# List of hyperparameters and their ranges\n",
    "hyperparameters = {\n",
    "    'colsample_bytree': [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1],\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {param: {'train_f1_scores': [], 'test_f1_scores': []} for param in hyperparameters}\n",
    "\n",
    "def evaluate_hyperparameter(param_name, param_values):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"Evaluating {param_name} with value {value}...\")\n",
    "        train_f1_scores = []\n",
    "        test_f1_scores = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 70,\n",
    "                'n_estimators': 85,\n",
    "                'max_depth': 30,\n",
    "                'min_child_samples': 4,\n",
    "                'subsample': 1.0,  # Default\n",
    "                'colsample_bytree': 1.0,  # Default\n",
    "                'scale_pos_weight': 1.5,\n",
    "                'reg_alpha': 1.2,\n",
    "                'reg_lambda': 1,\n",
    "                'random_state': 123\n",
    "            }\n",
    "            params[param_name] = value\n",
    "            \n",
    "            model = LGBMClassifier(**params)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_val_pred = model.predict(X_val_fold)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                train_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "                test_f1_scores.append(f1_score(y_test, y_test_pred, average='macro'))\n",
    "                \n",
    "                # Check the impact on training\n",
    "                print(f\"Trained with {param_name}={value} in {time.time() - start_time:.2f} seconds\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while training or predicting with {param_name}={value}: {e}\")\n",
    "\n",
    "        if train_f1_scores and test_f1_scores:\n",
    "            results[param_name]['train_f1_scores'].append(np.mean(train_f1_scores))\n",
    "            results[param_name]['test_f1_scores'].append(np.mean(test_f1_scores))\n",
    "        else:\n",
    "            print(f\"No scores collected for {param_name} with value {value}\")\n",
    "\n",
    "# Evaluate each hyperparameter\n",
    "for param_name, param_values in hyperparameters.items():\n",
    "    evaluate_hyperparameter(param_name, param_values)\n",
    "\n",
    "# Plotting results for each hyperparameter\n",
    "for param_name in hyperparameters:\n",
    "    if results[param_name]['train_f1_scores']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['train_f1_scores'], marker='o', label='Train F1 Score')\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['test_f1_scores'], marker='o', label='Test F1 Score')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Macro F1 Score vs. {param_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No results for hyperparameter {param_name}\")\n",
    "\n",
    "# Print results\n",
    "for param_name in hyperparameters:\n",
    "    print(f\"Hyperparameter: {param_name}\")\n",
    "    print(f\"Values: {hyperparameters[param_name]}\")\n",
    "    print(f\"Train F1 Scores: {results[param_name]['train_f1_scores']}\")\n",
    "    print(f\"Test F1 Scores: {results[param_name]['test_f1_scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0944f746",
   "metadata": {},
   "source": [
    "### Optimised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load your training dataset\n",
    "train_data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X_train = train_data.drop(['label', 'id'], axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "# Load your test dataset\n",
    "test_data = pd.read_csv('data/test_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column\n",
    "X_test = test_data.drop(['id'], axis=1).values\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Define model parameters\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 70,\n",
    "    'n_estimators': 85,\n",
    "    'max_depth': 30,\n",
    "    'min_child_samples': 4,\n",
    "    'subsample': 1.0,  # Default\n",
    "    'colsample_bytree': 0.55,\n",
    "    'scale_pos_weight': 1.5,\n",
    "    'reg_alpha': 1.2,\n",
    "    'reg_lambda': 1,\n",
    "    'random_state': 123\n",
    "}\n",
    "\n",
    "# Train the model on the entire training dataset\n",
    "model = LGBMClassifier(**params)\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Model trained in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Output the predictions\n",
    "output = pd.DataFrame({'id': test_data['id'], 'label': y_test_pred})\n",
    "#output.to_csv('Optimal_boosting.csv', index=False) #uncomment to get file, otherwise file is found in /task3_predictions\n",
    "\n",
    "\n",
    "print(\"Predictions saved to Optimal_boosting.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f155b0",
   "metadata": {},
   "source": [
    "The optimal hyperparameters are:\n",
    "- num_leaves: 70\n",
    "- n_estimators: 85\n",
    "- max_depth:30\n",
    "- min_child_samples: 4\n",
    "- colsample_bytree: 0.55\n",
    "- scale_pos_weight: 1.5\n",
    "- reg_alpha: 1\n",
    "- reg_lambda: 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc8c2e",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "Avengers Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test = pd.read_csv(\"Data/test_tfidf_features.csv\")\n",
    "\n",
    "# Read the first set of predictions\n",
    "y_pred1 = pd.read_csv(\"Task3_predictions/Optimal_LogReg.csv\")\n",
    "y_pred1 = y_pred1['label'].values\n",
    "\n",
    "# Read the second set of predictions\n",
    "y_pred2 = pd.read_csv(\"Task3_predictions/Optimal_gbdt.csv\")\n",
    "y_pred2 = y_pred2['label'].values\n",
    "\n",
    "# Read the third set of predictions\n",
    "y_pred3 = pd.read_csv(\"Task3_predictions/Optimal_forest.csv\")\n",
    "y_pred3 = y_pred3['label'].values\n",
    "\n",
    "# Read the fourth set of predictions\n",
    "y_pred4 = pd.read_csv(\"Task3_predictions/Optimal_NB.csv\")\n",
    "y_pred4 = y_pred4['label'].values\n",
    "\n",
    "# Read the fifth set of predictions\n",
    "y_pred5 = pd.read_csv(\"Task3_predictions/Optimal_svm.csv\")\n",
    "y_pred5 = y_pred5['label'].values\n",
    "\n",
    "# Combine predictions into a DataFrame for easier manipulation\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'pred1': y_pred1,\n",
    "    'pred2': y_pred2,\n",
    "    'pred3': y_pred3,\n",
    "    'pred4': y_pred4,\n",
    "    'pred5': y_pred5\n",
    "})\n",
    "\n",
    "# Perform majority voting\n",
    "predictions_df['label'] = (predictions_df[['pred1', 'pred2', 'pred3', 'pred4', 'pred5']].sum(axis=1) > 1).astype(int)\n",
    "\n",
    "#Display the first few rows of the final predictions\n",
    "print(predictions_df.head())\n",
    "\n",
    "# Save the final predictions to a CSV file\n",
    "final_df = predictions_df[['id', 'label']]\n",
    "#final_df.to_csv('final_predictions.csv', index=False) #uncomment to get file, otherwise file is found in root folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
