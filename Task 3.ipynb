{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8cbfdf",
   "metadata": {},
   "source": [
    "# Task 3: Try other machine learning models and race to the top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a0906",
   "metadata": {},
   "source": [
    "The final model used is made of 5 other models:\n",
    "1) Logistic regression\n",
    "\n",
    "2) SVM\n",
    "\n",
    "3) Naive Bayes\n",
    "\n",
    "4) Bagging\n",
    "\n",
    "5) Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666dc56",
   "metadata": {},
   "source": [
    "Each model will make their own prediction given the data point. The final model will then make its prediction based on the majority prediction of the 5 models.\n",
    "\n",
    "Each model creates their own prediction file that can be found in task3_predictions. Alternatively, we can uncomment the code that creates the file and run the code to obtain the predictions for each model.\n",
    "\n",
    "Let's see the code implementation of all 5 models and their hyperparameter optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a001c8a5",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a3978",
   "metadata": {},
   "source": [
    "The hyperparameters to optimise is Learning rate (lr) and Batch size (bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b5048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 0.6916, Validation F1: 0.3812, Validation Loss: 0.6875\n",
      "Epoch 100, Training Loss: 0.6604, Validation F1: 0.3812, Validation Loss: 0.6661\n",
      "Epoch 200, Training Loss: 0.6551, Validation F1: 0.3812, Validation Loss: 0.6663\n",
      "Epoch 300, Training Loss: 0.6501, Validation F1: 0.3821, Validation Loss: 0.6667\n",
      "Epoch 400, Training Loss: 0.6454, Validation F1: 0.3829, Validation Loss: 0.6672\n",
      "Epoch 0, Training Loss: 0.6946, Validation F1: 0.3812, Validation Loss: 0.6901\n",
      "Epoch 100, Training Loss: 0.6646, Validation F1: 0.3812, Validation Loss: 0.6660\n",
      "Epoch 200, Training Loss: 0.6618, Validation F1: 0.3812, Validation Loss: 0.6661\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17080\\2709861016.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;31m# Train the optimized bagging model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m \u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_f1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize_bagging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[0mbest_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17080\\2709861016.py\u001b[0m in \u001b[0;36moptimize_bagging\u001b[1;34m(X_features, Y_label)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlearning_rate_options\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_size_options\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_f1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17080\\2709861016.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, y, bs, epochs, lr, X_val, y_val)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mvalidation_f1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mvalidation_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17080\\2709861016.py\u001b[0m in \u001b[0;36mloss\u001b[1;34m(y, y_hat)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3438\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3440\u001b[1;33m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[0;32m   3441\u001b[0m                           out=out, **kwargs)\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         ret = um.true_divide(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define your train, predict, and accuracy functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "def gradient_descent(X, y, y_hat):\n",
    "    m = X.shape[0]\n",
    "    dw = (1 / m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1 / m) * np.sum(y_hat - y)\n",
    "    return dw, db\n",
    "\n",
    "def train(X, y, bs, epochs, lr, X_val, y_val):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros((n_features, 1))\n",
    "    b = 0\n",
    "    y = y.reshape(n_samples, 1)\n",
    "    \n",
    "    training_loss = []\n",
    "    validation_f1 = []\n",
    "    validation_loss = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, n_samples, bs):\n",
    "            X_batch = X[i:i + bs]\n",
    "            y_batch = y[i:i + bs]\n",
    "            y_hat = sigmoid(np.dot(X_batch, w) + b)\n",
    "            dw, db = gradient_descent(X_batch, y_batch, y_hat)\n",
    "            w -= lr * dw\n",
    "            b -= lr * db\n",
    "\n",
    "            epoch_loss += loss(y_batch, y_hat)\n",
    "        \n",
    "        epoch_loss /= (n_samples // bs)\n",
    "        training_loss.append(epoch_loss)\n",
    "\n",
    "        # Calculate validation F1 score and loss\n",
    "        y_hat_val = sigmoid(np.dot(X_val, w) + b)\n",
    "        y_pred_val = (y_hat_val > 0.5).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred_val, average='macro')\n",
    "        validation_f1.append(f1)\n",
    "        val_loss = loss(y_val, y_hat_val)\n",
    "        validation_loss.append(val_loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Training Loss: {epoch_loss:.4f}, Validation F1: {f1:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    return w, b, training_loss, validation_f1, validation_loss\n",
    "\n",
    "def predict(X, w, b):\n",
    "    y_hat = sigmoid(np.dot(X, w) + b)\n",
    "    pred = [1 if i > 0.5 else 0 for i in y_hat]\n",
    "    return np.array(pred)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "def optimize(X_features, Y_label):\n",
    "    epochs = 1000\n",
    "    learning_rate_options = [0.001, 0.01, 0.1, 0.2]\n",
    "    batch_size_options = [16, 32, 64]\n",
    "    k = 5  # Number of folds for cross-validation\n",
    "\n",
    "    f1_results = []\n",
    "    best_w = None\n",
    "    best_b = None\n",
    "    best_f1 = 0\n",
    "    best_params = {}\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    for lr in learning_rate_options:\n",
    "        for bs in batch_size_options:\n",
    "            f1_scores = []\n",
    "\n",
    "            for train_index, val_index in kf.split(X_features):\n",
    "                X_train_split, X_val_split = X_features[train_index], X_features[val_index]\n",
    "                y_train_split, y_val_split = Y_label[train_index], Y_label[val_index]\n",
    "                \n",
    "                w, b, training_loss, validation_f1, validation_loss = train(X_train_split, y_train_split, bs, epochs, lr, X_val_split, y_val_split)\n",
    "                y_pred = predict(X_val_split, w, b)\n",
    "                f1 = f1_score(y_val_split, y_pred, average='macro')\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "            avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                best_params = {'learning_rate': lr, 'batch_size': bs}\n",
    "                best_w = w\n",
    "                best_b = b\n",
    "            \n",
    "            # Store the results\n",
    "            f1_results.append((lr, bs, avg_f1))\n",
    "\n",
    "    f1_df = pd.DataFrame(f1_results, columns=['Learning Rate', 'Batch Size', 'F1 Score'])\n",
    "\n",
    "    # Identify the best hyperparameters\n",
    "    best_params_idx = f1_df['F1 Score'].idxmax()\n",
    "    best_params = f1_df.iloc[best_params_idx]\n",
    "\n",
    "    print(f'Best Hyperparameters: Learning Rate = {best_params[\"Learning Rate\"]}, Batch Size = {best_params[\"Batch Size\"]}, F1 Score = {best_params[\"F1 Score\"]:.4f}')\n",
    "\n",
    "    return best_params, f1_df, best_w, best_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data = pd.read_csv(\"Data/train_tfidf_features.csv\")\n",
    "X_features = data.drop(['label', 'id'], axis=1).values\n",
    "Y_label = data['label'].values\n",
    "\n",
    "# Optimize hyperparameters\n",
    "best_params, f1_df, best_w, best_b = optimize(X_features, Y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f07a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = best_params['Learning Rate']\n",
    "best_bs = best_params['Batch Size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_vs_batch_size(f1_df):\n",
    "    learning_rate_options = f1_df['Learning Rate'].unique()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for lr in learning_rate_options:\n",
    "        subset = f1_df[f1_df['Learning Rate'] == lr]\n",
    "        plt.plot(subset['Batch Size'], subset['F1 Score'], marker='o', label=f'LR: {lr}')\n",
    "    plt.title('F1 Score vs Batch Size')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_f1_vs_learning_rate(f1_df):\n",
    "    batch_size_options = f1_df['Batch Size'].unique()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for bs in batch_size_options:\n",
    "        subset = f1_df[f1_df['Batch Size'] == bs]\n",
    "        plt.plot(subset['Learning Rate'], subset['F1 Score'], marker='o', label=f'BS: {bs}')\n",
    "    plt.title('F1 Score vs Learning Rate')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_pred_vs_actual(y_val, y_pred):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(range(len(y_val)), y_val, color='blue', alpha=0.6, label='Actual')\n",
    "    plt.scatter(range(len(y_pred)), y_pred, color='red', alpha=0.6, label='Predicted')\n",
    "    plt.plot(range(len(y_pred)), y_pred, color='red', alpha=0.6)\n",
    "    plt.title('Predicted vs Actual Values')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Class Label')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035dd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plot_f1_vs_batch_size(f1_df)\n",
    "plot_f1_vs_learning_rate(f1_df)\n",
    "\n",
    "# Validate with the best parameters\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_features, Y_label, test_size=0.2, random_state=42)\n",
    "y_pred = predict(X_val_split, best_w, best_b)\n",
    "\n",
    "# Plot predicted vs actual values\n",
    "plot_pred_vs_actual(y_val_split, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bfbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data and first set of predictions\n",
    "test = pd.read_csv(\"Data/test_tfidf_features.csv\")\n",
    "\n",
    "# Predict on the test data using the best model\n",
    "X_test = test.drop(columns=['id']).values\n",
    "y_pred_best = predict(X_test, best_w, best_b)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'label': y_pred_best\n",
    "})\n",
    "\n",
    "# Display the first few rows of the final predictions\n",
    "print(predictions_df.head())\n",
    "\n",
    "#predictions_df.to_csv('Optimal_LogReg.csv', index=False) #uncomment to get file, otherwise, predictions can be found in task3_predictions\n",
    "\n",
    "# Completed Print\n",
    "print(\"Final predictions saved to logreg_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0acd4",
   "metadata": {},
   "source": [
    "The optimal hyperparater values are\n",
    "Learning rate: 0.1\n",
    "Batch sizeL: 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30edd0",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0fec0",
   "metadata": {},
   "source": [
    "SVM with different kernel functions were investigated:\n",
    "- Linear SVM (investigated both LinearSVC and SVC module)\n",
    "- Radial Basis function SVM*\n",
    "- Sigmoid SVM\n",
    "- Poly SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e9bbb",
   "metadata": {},
   "source": [
    "#### Linear SVM\n",
    "\n",
    "Using SVC module\n",
    "\n",
    "Hyperparameters:\n",
    "1) C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21340d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "X = data.drop(columns=['id', 'label']).values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PLAIN AND SIMPLE LINEAR SVM USING SVC KERNEL = LINEAR\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('svc', SVC(kernel='linear',\n",
    "               gamma=0.1))\n",
    "])\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10, 50, 100]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro') \n",
    "\n",
    "\n",
    "print(f'Accuracy with SGDClassifier: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report with SGDClassifier:')\n",
    "print(report)\n",
    "\n",
    "# Result\n",
    "# F1: 0.6926448445963982"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a5835",
   "metadata": {},
   "source": [
    "Using LinearSVC module,\n",
    "\n",
    "Hyperparameters:\n",
    "1) C\n",
    "2) Max Iteration\n",
    "3) loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR SVM WITH DIFFERING REGULARISATION VALUE USING LINEARSVC\n",
    "# https://scikit-learn.org/stable/auto_examples/svm/plot_svm_scale_c.html#sphx-glr-auto-examples-svm-plot-svm-scale-c-py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "\"\"\" import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "X = data.drop(columns=['id', 'label']).values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# uncomment in case prev is disrupted \"\"\"\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('svc', LinearSVC(penalty=\"l1\", loss=\"squared_hinge\", dual=False, tol=1e-3))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svc__C': [0.17385, 0.173875, 0.1739, 0.173925, 0.17395],\n",
    "    'svc__max_iter': [6000, 6200, 6300, 6400, 6500],\n",
    "    'svc__loss': ['hinge','squared_hinge']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=7, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f'Best parameters: {best_params}')\n",
    "\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "\n",
    "\"\"\" Best parameters: {'svc__C': 0.17385, 'svc__loss': 'squared_hinge', 'svc__max_iter': 6000}\n",
    "Accuracy: 0.722432353796916\n",
    "F1 Score: 0.6880534155931336 with cv=7.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d51eb",
   "metadata": {},
   "source": [
    "#### Sigmoid SVM\n",
    "\n",
    "Hyperparameters:\n",
    "- C: regularisation term\n",
    "- coef: independent term added to the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdaa2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid SVM\n",
    "# https://stats.stackexchange.com/questions/90736/the-difference-of-kernels-in-svm\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score,f1_score\n",
    "\n",
    "\"\"\" import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "X = data.drop(columns=['id', 'label']).values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# uncomment in case prev is disrupted \"\"\"\n",
    "\n",
    "\n",
    "svm_sigmoid = SVC(kernel='sigmoid', C=1.0, coef0=1)\n",
    "svm_sigmoid.fit(X_train, y_train)\n",
    "y_pred = svm_sigmoid.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "'''\n",
    "Accuracy: 0.5292406168169915\n",
    "F1 Score: 0.49756365081946474\n",
    "Noting the poor performance, sigmoid SVM was not investigated further\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b532fce6",
   "metadata": {},
   "source": [
    "#### Poly SVM\n",
    "\n",
    "Hyperparameters:\n",
    "- C: regularisation term\n",
    "- coef: independent term added to the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a346adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLY SVM\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "\"\"\" import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "X = data.drop(columns=['id', 'label']).values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# uncomment in case prev is disrupted \"\"\"\n",
    "\n",
    "svm_poly = SVC(kernel='poly', degree=5, C=1.0, coef0=1) # here we will explore different degrees\n",
    "\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "'''\n",
    "Accuracy: 0.6264183881291824\n",
    "F1 Score: 0.3851520572450805\n",
    "Noting the poor performance, Poly SVM was not investigated further\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf2cffd",
   "metadata": {},
   "source": [
    "#### Radial basis function SVM \n",
    "\n",
    "This was chosen as the final model for SVM.\n",
    "\n",
    "Hyperparamters:\n",
    "- c: regularisation term \n",
    "- gamma: kernel coefficient \n",
    "\n",
    "Additional Reference for the rationale of choosing RBF:\n",
    "https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radial Basis function SVM\n",
    "# https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "X = data.drop(columns=['id', 'label']).values\n",
    "Y = data['label'].values\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score,f1_score\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('rbf', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'rbf__C': [ 5, 10, 20,30, 50],\n",
    "    'rbf__gamma': ['scale', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1_macro', n_jobs=-1) \n",
    "\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "print(f'Best parameters: {best_params}')\n",
    "\n",
    "# Modified this cell to train with 100% of training data to enhance the model's performance, to better perform of predicting test data.\n",
    "# Previous modification of the cell utilised 80-20% split, the following was its performance\n",
    "# F1 score was a 0.69934251368587 with gamma= 'scale' and C = 10\n",
    "# F1 score was a 0.7002338328283246 with gamma= 1 and C = 5\n",
    "# However took 300 mins to train each time......."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36be52",
   "metadata": {},
   "source": [
    "Predicting Test Data using RBF SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2be069",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('Data/test_tfidf_features.csv')\n",
    "x_test_dataset = test_dataset.drop(columns=['id']).values\n",
    "y_test_pred = best_model.predict(x_test_dataset)\n",
    "predictions_df = pd.DataFrame({'id': test_dataset['id'], 'label': y_test_pred})\n",
    "# Save the DataFrame to a CSV file\n",
    "#predictions_df.to_csv('Optimal_svm.csv', index=False)  #uncomment to get file, otherwise file is found in /task3_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240dc61f",
   "metadata": {},
   "source": [
    "Validation of SVM using RBF Kernel compared to other Kernels:\n",
    "\n",
    "Compare performance with other kernels to validate that it is better than other kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a991c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "\"\"\" import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "X = data.drop(columns=['id', 'label']).values\n",
    "y = data['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# uncomment in case prev is disrupted \"\"\"\n",
    "\n",
    "\n",
    "svm = SVC()\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectKBest(chi2)), \n",
    "    ('rbf', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classification__C': np.logspace(-3, 3, 7),\n",
    "    'classification__kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "    'classification__gamma': ['scale', 0.001, 0.01, 0.1, 1],\n",
    "    'feature_selection__k': [3000, 4000, 5000]\n",
    "}\n",
    "\n",
    "#cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f'Best parameters: {best_params}')\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {macro_f1}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "'''\n",
    "\n",
    "Best parameters: {'feature_selection__k': 5000, 'rbf__C': 5, 'rbf__gamma': 1}\n",
    "Accuracy: 0.7299970904858889\n",
    "F1 Score: 0.7002338328283246\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.76      0.83      0.79      2153\n",
    "           1       0.67      0.56      0.61      1284\n",
    "\n",
    "    accuracy                           0.73      3437\n",
    "   macro avg       0.71      0.69      0.70      3437\n",
    "weighted avg       0.72      0.73      0.72      3437\n",
    "\n",
    "Note this will take 600 mins\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab26695",
   "metadata": {},
   "source": [
    "Based on the investigation above, both the Linear SVM and RBF SVM models achieved similar accuracy and F1 scores, with the RBF SVM having slightly higher values. However, the RBF SVM takes significantly more time to train, requiring 30 minutes compared to just 2 minutes for the Linear SVM. Ultimately, the SVM with the RBF kernel was chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106615ea",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb848a",
   "metadata": {},
   "source": [
    "The hyperparameter optimised is alpha, also known as smoothing factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229d0099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, F1 Score: 0.6706924170645149\n",
      "Alpha: 0.02, F1 Score: 0.6726259323487608\n",
      "Alpha: 0.03, F1 Score: 0.6727110494395248\n",
      "Alpha: 0.04, F1 Score: 0.6735146478858398\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3616\\3666312351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malpha_values\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mcv_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Alpha: {alpha}, F1 Score: {np.mean(cv_scores)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m     cv_results = cross_validate(\n\u001b[0m\u001b[0;32m    510\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# independent, and that it is pickle-able.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m     results = parallel(\n\u001b[0m\u001b[0;32m    268\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    269\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m_safe_split\u001b[1;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_pandas_indexing\u001b[1;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;31m# using take() instead of iloc[] ensures the return value is a \"proper\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;31m# copy that will not raise SettingWithCopyWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;31m# check whether we should index with loc or iloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3701\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3703\u001b[1;33m         new_data = self._mgr.take(\n\u001b[0m\u001b[0;32m   3704\u001b[0m             \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3705\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m         return self.reindex_indexer(\n\u001b[0m\u001b[0;32m    898\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    687\u001b[0m             )\n\u001b[0;32m    688\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             new_blocks = [\n\u001b[0m\u001b[0;32m    690\u001b[0m                 blk.take_nd(\n\u001b[0;32m    691\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             new_blocks = [\n\u001b[1;32m--> 690\u001b[1;33m                 blk.take_nd(\n\u001b[0m\u001b[0;32m    691\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                     \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[0mallow_fill\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m         new_values = algos.take_nd(\n\u001b[0m\u001b[0;32m   1140\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\pandas\\core\\array_algos\\take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_take_nd_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, make_scorer\n",
    "import joblib\n",
    "\n",
    "# Loading the training data\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "X = data.drop(columns=['id', 'label'])\n",
    "Y = data['label']\n",
    "\n",
    "\n",
    "'''\n",
    "MultinomialNB has 2 main parameters (alpha and fit_prior)\n",
    "alpha is the smoothing parameter, preventing the model from assigning 0 probability to unseen words\n",
    "adds alpha to count when calculating conditional probability. (default =1)\n",
    "fit_prior is a boolean that sets whether the model will calculate prior probabilities from the training set. (default = True)\n",
    "Also important to note that SciKit uses log likelihood when calculating \n",
    "'''\n",
    "#Set Up a Detailed Hyperparameter Grid for Alpha\n",
    "alpha_values = np.arange(0.01, 1, 0.01)  # More detailed range of alpha values\n",
    "\n",
    "#Perform K-Fold Cross-Validation for Each Alpha\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "results = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    model = MultinomialNB(alpha=alpha)\n",
    "    cv_scores = cross_val_score(model, X, Y, cv=kf, scoring=scorer)\n",
    "    results.append((alpha, np.mean(cv_scores)))\n",
    "    print(f\"Alpha: {alpha}, F1 Score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Convert results to a DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(results, columns=['Alpha', 'F1 Score'])\n",
    "\n",
    "#Plot the Results to Find the Optimal Alpha\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['Alpha'], results_df['F1 Score'], marker='o')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('F1 Score (Macro)')\n",
    "plt.title('Alpha vs. F1 Score (Macro)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#Find the Exact Optimal Alpha Value\n",
    "optimal_alpha = results_df.loc[results_df['F1 Score'].idxmax()]['Alpha']\n",
    "optimal_f1_score = results_df.loc[results_df['F1 Score'].idxmax()]['F1 Score']\n",
    "\n",
    "print(f\"Optimal alpha value: {optimal_alpha}\")\n",
    "print(f\"F1 Score at optimal alpha: {optimal_f1_score}\")\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = MultinomialNB(alpha = 0.26)\n",
    "\n",
    "# Load the test data\n",
    "test = pd.read_csv(\"Data/test_tfidf_features.csv\")\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "\n",
    "# Fit the model on the entire training data\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Create a DataFrame with predictions\n",
    "predictions_df = pd.DataFrame({'id': test['id'], 'label': y_test_pred})\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "#predictions_df.to_csv('Optimal_NB.csv', index=False) #uncomment to get file, otherwise file is found in /task3_predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039995b",
   "metadata": {},
   "source": [
    "The optimal alpha value is alpha = 0.26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81987095",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23d783",
   "metadata": {},
   "source": [
    "Bagging was investigated using SKlearn's random forest classifier.\n",
    "\n",
    "The hyperparameters to optimise were \n",
    "- n_estimators: The number of trees in the forest.\n",
    "- Criterion: The function to measure the quality of a split.\n",
    "- max_depth: The maximum depth of the tree.  \n",
    "- min_samples_split: The minimum number of samples required to split an internal node\n",
    "- min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "- max_features: The number of features to consider when looking for the best split\n",
    "- max_samples: The number of samples to draw to train each tree.\n",
    "- class_weight: Weights associated with classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "df = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "k = 5\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=['id', 'label'])\n",
    "y = df['label']\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=400, random_state=42, criterion=\"entropy\", min_samples_split=80, max_features='sqrt', max_samples=0.6, \n",
    "                               class_weight='balanced_subsample', max_depth=150, min_samples_leaf=2)\n",
    "\n",
    "if False:  # Change this to True to activate hyperparameter optimization\n",
    "    param_grid = {\n",
    "        'n_estimators': [350,360,370,380,390]\n",
    "    }\n",
    "    \n",
    "    print(\"Doing hyperparameter optimization\")\n",
    "    halving_grid_search = HalvingGridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        factor=3,  # The factor by which the number of candidates is reduced at each iteration\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    halving_grid_search.fit(X_train, y_train)\n",
    "    print(\"Best parameters found: \", halving_grid_search.best_params_)\n",
    "\n",
    "    # Collecting results\n",
    "    results = pd.DataFrame(halving_grid_search.cv_results_)\n",
    "\n",
    "    # Best model evaluation\n",
    "    best_model = halving_grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=1)\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "if True:\n",
    "    print(\"Doing Cross Validation\")\n",
    "    scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "    print(f\"Cross-Validation F1 Scores for {k} folds: {scores}\")\n",
    "    print(\"Mean Accuracy:\", scores.mean())\n",
    "    print(\"Standard Deviation:\", scores.std())\n",
    "\n",
    "if True:\n",
    "    test = pd.read_csv(\"Data/test_tfidf_features.csv\")\n",
    "    X_test = test.drop(['id'], axis=1).values\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions_df = pd.DataFrame({'id': test['id'], 'label': y_pred})\n",
    "\n",
    "    # Display the first few rows of the predictions\n",
    "    print(predictions_df.head())\n",
    "\n",
    "    # Save predictions to a CSV file\n",
    "    #predictions_df.to_csv('Optimal_bagging', index=False) #uncomment to get file, otherwise file is found in /task3_predictions\n",
    "\n",
    "\n",
    "if False: #creating graphs for each hyperparameter\n",
    "    min_samples_split_range = range(2, 100, 1)\n",
    "\n",
    "    # Store the mean and standard deviation of the F1 Macro scores for each n_estimators\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for min_samples_split in min_samples_split_range:\n",
    "        print(f\"Evaluating model with min_samples_split={min_samples_split}\")\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, criterion=\"entropy\",\n",
    "                                       min_samples_split=min_samples_split, max_features='sqrt', max_samples=0.7,\n",
    "                                       class_weight='balanced_subsample', max_depth=80, min_samples_leaf=1)\n",
    "        scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "        mean_scores.append(scores.mean())\n",
    "        std_scores.append(scores.std())\n",
    "        print(f\"Mean F1 Macro Score: {scores.mean()} | Std Dev: {scores.std()}\")\n",
    "\n",
    "    # Save the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(min_samples_split_range, mean_scores, yerr=std_scores, fmt='-o')\n",
    "    plt.xlabel('min_samples_split')\n",
    "    plt.ylabel('Mean F1 Macro Score')\n",
    "    plt.title('F1 Macro Score vs. min_samples_split')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('min_samples_split_vs_f1_macro.png')\n",
    "    plt.close()\n",
    "\n",
    "    max_depth_range = range(1, 200, 1)\n",
    "\n",
    "    # Store the mean and standard deviation of the F1 Macro scores for each n_estimators\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for max_depth in max_depth_range:\n",
    "        print(f\"Evaluating model with max_depth={max_depth}\")\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, criterion=\"entropy\",\n",
    "                                       min_samples_split=7, max_features='sqrt', max_samples=0.7,\n",
    "                                       class_weight='balanced_subsample', max_depth=max_depth, min_samples_leaf=1)\n",
    "        scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "        mean_scores.append(scores.mean())\n",
    "        std_scores.append(scores.std())\n",
    "        print(f\"Mean F1 Macro Score: {scores.mean()} | Std Dev: {scores.std()}\")\n",
    "\n",
    "    # Save the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(max_depth_range, mean_scores, yerr=std_scores, fmt='-o')\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('Mean F1 Macro Score')\n",
    "    plt.title('F1 Macro Score vs. max_depth')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('max_depth_vs_f1_macro.png')\n",
    "    plt.close()\n",
    "\n",
    "    min_samples_leaf_range = range(1, 100, 1)\n",
    "\n",
    "    # Store the mean and standard deviation of the F1 Macro scores for each n_estimators\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for min_samples_leaf in min_samples_leaf_range:\n",
    "        print(f\"Evaluating model with min_samples_leaf={min_samples_leaf}\")\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, criterion=\"entropy\",\n",
    "                                       min_samples_split=7, max_features='sqrt', max_samples=0.7,\n",
    "                                       class_weight='balanced_subsample', max_depth=80, min_samples_leaf=min_samples_leaf)\n",
    "        scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "        mean_scores.append(scores.mean())\n",
    "        std_scores.append(scores.std())\n",
    "        print(f\"Mean F1 Macro Score: {scores.mean()} | Std Dev: {scores.std()}\")\n",
    "\n",
    "    # Save the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(min_samples_leaf_range, mean_scores, yerr=std_scores, fmt='-o')\n",
    "    plt.xlabel('min_samples_leaf')\n",
    "    plt.ylabel('Mean F1 Macro Score')\n",
    "    plt.title('F1 Macro Score vs. min_samples_leaf')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('min_samples_leaf_vs_f1_macro.png')\n",
    "    plt.close()\n",
    "\n",
    "    max_samples_range = np.arange(0.01, 1, 0.01)\n",
    "\n",
    "    # Store the mean and standard deviation of the F1 Macro scores for each n_estimators\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for max_samples in max_samples_range:\n",
    "        print(f\"Evaluating model with max_samples={max_samples}\")\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, criterion=\"entropy\",\n",
    "                                       min_samples_split=7, max_features='sqrt', max_samples=max_samples,\n",
    "                                       class_weight='balanced_subsample', max_depth=80, min_samples_leaf=1)\n",
    "        scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "        mean_scores.append(scores.mean())\n",
    "        std_scores.append(scores.std())\n",
    "        print(f\"Mean F1 Macro Score: {scores.mean()} | Std Dev: {scores.std()}\")\n",
    "\n",
    "    # Save the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(max_samples_range, mean_scores, yerr=std_scores, fmt='-o')\n",
    "    plt.xlabel('max_samples')\n",
    "    plt.ylabel('Mean F1 Macro Score')\n",
    "    plt.title('F1 Macro Score vs. max_samples')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('max_samples_vs_f1_macro.png')\n",
    "    plt.close()\n",
    "\n",
    "    n_estimators_range = range(10, 510, 10)\n",
    "\n",
    "    # Store the mean and standard deviation of the F1 Macro scores for each n_estimators\n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for n_estimators in n_estimators_range:\n",
    "        print(f\"Evaluating model with n_estimators={n_estimators}\")\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, random_state=42, criterion=\"entropy\",\n",
    "                                       min_samples_split=7, max_features='sqrt', max_samples=0.7,\n",
    "                                       class_weight='balanced_subsample', max_depth=80, min_samples_leaf=1)\n",
    "        scores = cross_val_score(model, X, y, cv=k, scoring='f1_macro')\n",
    "        mean_scores.append(scores.mean())\n",
    "        std_scores.append(scores.std())\n",
    "        print(f\"Mean F1 Macro Score: {scores.mean()} | Std Dev: {scores.std()}\")\n",
    "\n",
    "    # Save the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(n_estimators_range, mean_scores, yerr=std_scores, fmt='-o')\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('Mean F1 Macro Score')\n",
    "    plt.title('F1 Macro Score vs. n_estimators')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('n_estimators_vs_f1_macro.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cc195",
   "metadata": {},
   "source": [
    "The optimal values for each hyperparameters are:\n",
    "- n_estimators: 400\n",
    "- Criterion: \"entropy\" \n",
    "- max_depth: 150  \n",
    "- min_samples_split: 80\n",
    "- min_samples_leaf: 2\n",
    "- max_features: 'sqrt'\n",
    "- max_samples: 0.6\n",
    "- class_weight: 'balanced_subsample'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e4dd95",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630a44c",
   "metadata": {},
   "source": [
    "Boosting was investigated using Light Gradient Boosting Machine (LightGBM)\n",
    "\n",
    "The hyperparameters to optimise were\n",
    "- num_leaves: The maximum number of leaves in one tree\n",
    "- n_estimators: The number of boosting iterations (trees).\n",
    "- max_depth: The maximum depth of each tree.\n",
    "- min_child_samples: The minimum number of data points needed in a leaf.\n",
    "- colsample_bytree: The fraction of features to consider when building each tree.\n",
    "- scale_pos_weight: The weight for balancing the positive and negative classes in the binary classification task.\n",
    "- reg_alpha:  L1 regularization term on weights\n",
    "- reg_lambda: L2 regularization term on weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac22dc",
   "metadata": {},
   "source": [
    "Deciding between models:\n",
    "- Decision Stump\n",
    "- Logistic Regression\n",
    "- Linear SVM\n",
    "- Gaussian Naive Bayes\n",
    "- GBDT\n",
    "- DART\n",
    "- GOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3977fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values  # Convert to NumPy array\n",
    "y = data['label'].values  # Convert to NumPy array\n",
    "\n",
    "# List of random seeds\n",
    "random_seeds = [42, 52, 62]\n",
    "\n",
    "# List of boosting algorithms and weak learners\n",
    "boosting_algorithms = {\n",
    "    'LightGBM GBDT': LGBMClassifier(boosting_type='gbdt'),\n",
    "    'LightGBM DART': LGBMClassifier(boosting_type='dart'),\n",
    "    'LightGBM GOSS': LGBMClassifier(boosting_type='goss')\n",
    "}\n",
    "\n",
    "weak_learners = {\n",
    "    'Decision Stump': DecisionTreeClassifier(max_depth=1),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Gaussian Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store F1 scores and times for each algorithm/learner\n",
    "boosting_results = {name: {'f1_scores': [], 'times': []} for name in boosting_algorithms}\n",
    "weak_learner_results = {name: {'f1_scores': [], 'times': []} for name in weak_learners}\n",
    "\n",
    "def evaluate_boosting(name, model, seed):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Initialize the model\n",
    "    model.set_params(random_state=seed)\n",
    "    \n",
    "    # Measure the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Measure the end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the time taken\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the macro F1 score on the test set\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    # Return the F1 score and time taken\n",
    "    return name, seed, f1, elapsed_time\n",
    "\n",
    "def evaluate_learner(name, base_estimator, seed):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Initialize the AdaBoost classifier\n",
    "    adaboost = AdaBoostClassifier(estimator=base_estimator, n_estimators=10, learning_rate=1.0, random_state=seed)\n",
    "    \n",
    "    # Measure the start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the classifier\n",
    "    adaboost.fit(X_train, y_train)\n",
    "    \n",
    "    # Measure the end time\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the time taken\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = adaboost.predict(X_test)\n",
    "    \n",
    "    # Calculate the macro F1 score on the test set\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    # Return the F1 score and time taken\n",
    "    return name, seed, f1, elapsed_time\n",
    "\n",
    "# Use Parallel and delayed to parallelize the evaluation\n",
    "results_boosting = Parallel(n_jobs=-1)(delayed(evaluate_boosting)(name, model, seed)\n",
    "                                       for name, model in boosting_algorithms.items()\n",
    "                                       for seed in random_seeds)\n",
    "\n",
    "results_weak_learners = Parallel(n_jobs=-1)(delayed(evaluate_learner)(name, base_estimator, seed)\n",
    "                                            for name, base_estimator in weak_learners.items()\n",
    "                                            for seed in random_seeds)\n",
    "\n",
    "# Process results for boosting algorithms\n",
    "for name, seed, f1, elapsed_time in results_boosting:\n",
    "    boosting_results[name]['f1_scores'].append(f1)\n",
    "    boosting_results[name]['times'].append(elapsed_time)\n",
    "\n",
    "# Process results for weak learners\n",
    "for name, seed, f1, elapsed_time in results_weak_learners:\n",
    "    weak_learner_results[name]['f1_scores'].append(f1)\n",
    "    weak_learner_results[name]['times'].append(elapsed_time)\n",
    "\n",
    "# Calculate the mean and standard deviation of F1 scores and times for each algorithm/learner\n",
    "boosting_means_f1 = {name: np.mean(boosting_results[name]['f1_scores']) for name in boosting_algorithms}\n",
    "boosting_stds_f1 = {name: np.std(boosting_results[name]['f1_scores']) for name in boosting_algorithms}\n",
    "boosting_means_time = {name: np.mean(boosting_results[name]['times']) for name in boosting_algorithms}\n",
    "boosting_stds_time = {name: np.std(boosting_results[name]['times']) for name in boosting_algorithms}\n",
    "\n",
    "learner_means_f1 = {name: np.mean(weak_learner_results[name]['f1_scores']) for name in weak_learners}\n",
    "learner_stds_f1 = {name: np.std(weak_learner_results[name]['f1_scores']) for name in weak_learners}\n",
    "learner_means_time = {name: np.mean(weak_learner_results[name]['times']) for name in weak_learners}\n",
    "learner_stds_time = {name: np.std(weak_learner_results[name]['times']) for name in weak_learners}\n",
    "\n",
    "# Combine the results for plotting\n",
    "all_names = list(boosting_means_f1.keys()) + list(learner_means_f1.keys())\n",
    "all_means_f1 = list(boosting_means_f1.values()) + list(learner_means_f1.values())\n",
    "all_stds_f1 = list(boosting_stds_f1.values()) + list(learner_stds_f1.values())\n",
    "all_means_time = list(boosting_means_time.values()) + list(learner_means_time.values())\n",
    "all_stds_time = list(boosting_stds_time.values()) + list(learner_stds_time.values())\n",
    "\n",
    "# Create a dual dot plot\n",
    "fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot F1 scores\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Algorithm / Weak Learner')\n",
    "ax1.set_ylabel('Mean Macro F1 Score', color=color)\n",
    "ax1.errorbar(all_names, all_means_f1, yerr=all_stds_f1, fmt='o', capsize=5, color=color, label='Mean F1 Score')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a second y-axis to plot times\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Mean Time Taken (seconds)', color=color)\n",
    "ax2.errorbar(all_names, all_means_time, yerr=all_stds_time, fmt='o', capsize=5, color=color, label='Mean Time Taken')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Add a title and grid\n",
    "plt.title('Mean Macro F1 Score and Mean Time Taken vs. Algorithm with Error Bars')\n",
    "ax1.grid(True)\n",
    "fig.tight_layout()\n",
    "plt.xticks(rotation=45)\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Print the mean and standard deviation of F1 scores and times for each algorithm/learner\n",
    "for name in all_names:\n",
    "    if name in boosting_means_f1:\n",
    "        print(f\"Boosting Algorithm: {name}, Mean F1 Score: {boosting_means_f1[name]:.2f}, Std: {boosting_stds_f1[name]:.2f}\")\n",
    "        print(f\"Boosting Algorithm: {name}, Mean Time Taken: {boosting_means_time[name]:.2f} seconds, Std: {boosting_stds_time[name]:.2f}\")\n",
    "    else:\n",
    "        print(f\"Weak Learner: {name}, Mean F1 Score: {learner_means_f1[name]:.2f}, Std: {learner_stds_f1[name]:.2f}\")\n",
    "        print(f\"Weak Learner: {name}, Mean Time Taken: {learner_means_time[name]:.2f} seconds, Std: {learner_stds_time[name]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63ccb8",
   "metadata": {},
   "source": [
    "GBDT was selected as the model to undergo further optimisation of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209b677",
   "metadata": {},
   "source": [
    "#### Optimisation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f072f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# List of hyperparameters and their ranges\n",
    "hyperparameters = {\n",
    "    'num_leaves': [15, 31, 63, 127, 255],\n",
    "    'n_estimators': [50, 100, 200, 400, 800],\n",
    "    'max_depth': [-1, 3, 5, 7, 9, 12],\n",
    "    'min_child_samples': [10, 20, 50, 100, 150],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'scale_pos_weight': [1, 2, 5, 10, 20],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1, 2],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1, 2]\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {param: {'train_f1_scores': [], 'test_f1_scores': []} for param in hyperparameters}\n",
    "\n",
    "def evaluate_hyperparameter(param_name, param_values):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"Evaluating {param_name} with value {value}...\")\n",
    "        train_f1_scores = []\n",
    "        test_f1_scores = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 31,  # Default\n",
    "                'n_estimators': 100,  # Default\n",
    "                'max_depth': -1,  # Default\n",
    "                'min_child_samples': 20,  # Default\n",
    "                'subsample': 1.0,  # Default\n",
    "                'colsample_bytree': 1.0,  # Default\n",
    "                'scale_pos_weight': 1,  # Default\n",
    "                'reg_alpha': 0,  # Default\n",
    "                'reg_lambda': 0,  # Default\n",
    "                'random_state': 123\n",
    "            }\n",
    "            params[param_name] = value\n",
    "            \n",
    "            model = LGBMClassifier(**params)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_val_pred = model.predict(X_val_fold)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                train_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "                test_f1_scores.append(f1_score(y_test, y_test_pred, average='macro'))\n",
    "            except Exception as e:\n",
    "                print(f\"Error while training or predicting with {param_name}={value}: {e}\")\n",
    "\n",
    "        if train_f1_scores and test_f1_scores:\n",
    "            results[param_name]['train_f1_scores'].append(np.mean(train_f1_scores))\n",
    "            results[param_name]['test_f1_scores'].append(np.mean(test_f1_scores))\n",
    "        else:\n",
    "            print(f\"No scores collected for {param_name} with value {value}\")\n",
    "\n",
    "# Evaluate each hyperparameter\n",
    "for param_name, param_values in hyperparameters.items():\n",
    "    evaluate_hyperparameter(param_name, param_values)\n",
    "\n",
    "# Plotting results for each hyperparameter\n",
    "for param_name in hyperparameters:\n",
    "    if results[param_name]['train_f1_scores']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['train_f1_scores'], marker='o', label='Train F1 Score')\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['test_f1_scores'], marker='o', label='Test F1 Score')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Macro F1 Score vs. {param_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No results for hyperparameter {param_name}\")\n",
    "\n",
    "# Print results\n",
    "for param_name in hyperparameters:\n",
    "    print(f\"Hyperparameter: {param_name}\")\n",
    "    print(f\"Values: {hyperparameters[param_name]}\")\n",
    "    print(f\"Train F1 Scores: {results[param_name]['train_f1_scores']}\")\n",
    "    print(f\"Test F1 Scores: {results[param_name]['test_f1_scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8f79a",
   "metadata": {},
   "source": [
    "#### Optimisation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# List of hyperparameters and their ranges\n",
    "hyperparameters = {\n",
    "    'num_leaves': [31, 40, 50, 63, 70, 80, 90, 100],\n",
    "    'n_estimators': [75, 100, 125, 150, 175, 200],\n",
    "    'max_depth': [-1, 10, 20, 30, 40, 50, 60, 70],\n",
    "    'min_child_samples': [1, 4, 8, 12, 16, 20],\n",
    "    'scale_pos_weight': [1, 1.25, 1.5, 1.75, 2, 2.25, 2.5],\n",
    "    'reg_alpha': [0, 0.4, 0.8, 1.2, 1.6, 2],\n",
    "    'reg_lambda': [0, 0.4, 0.8, 1.2, 1.6, 2]\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {param: {'train_f1_scores': [], 'test_f1_scores': []} for param in hyperparameters}\n",
    "\n",
    "def evaluate_hyperparameter(param_name, param_values):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"Evaluating {param_name} with value {value}...\")\n",
    "        train_f1_scores = []\n",
    "        test_f1_scores = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 63,  # Default\n",
    "                'n_estimators': 100,  # Default\n",
    "                'max_depth': -1,  # Default\n",
    "                'min_child_samples': 10,  # Default\n",
    "                'subsample': 1.0,  # Default\n",
    "                'colsample_bytree': 1.0,  # Default\n",
    "                'scale_pos_weight': 1,  # Default\n",
    "                'reg_alpha': 0,  # Default\n",
    "                'reg_lambda': 0,  # Default\n",
    "                'random_state': 123\n",
    "            }\n",
    "            params[param_name] = value\n",
    "            \n",
    "            model = LGBMClassifier(**params)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_val_pred = model.predict(X_val_fold)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                train_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "                test_f1_scores.append(f1_score(y_test, y_test_pred, average='macro'))\n",
    "            except Exception as e:\n",
    "                print(f\"Error while training or predicting with {param_name}={value}: {e}\")\n",
    "\n",
    "        if train_f1_scores and test_f1_scores:\n",
    "            results[param_name]['train_f1_scores'].append(np.mean(train_f1_scores))\n",
    "            results[param_name]['test_f1_scores'].append(np.mean(test_f1_scores))\n",
    "        else:\n",
    "            print(f\"No scores collected for {param_name} with value {value}\")\n",
    "\n",
    "# Evaluate each hyperparameter\n",
    "for param_name, param_values in hyperparameters.items():\n",
    "    evaluate_hyperparameter(param_name, param_values)\n",
    "\n",
    "# Plotting results for each hyperparameter\n",
    "for param_name in hyperparameters:\n",
    "    if results[param_name]['train_f1_scores']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['train_f1_scores'], marker='o', label='Train F1 Score')\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['test_f1_scores'], marker='o', label='Test F1 Score')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Macro F1 Score vs. {param_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No results for hyperparameter {param_name}\")\n",
    "\n",
    "# Print results\n",
    "for param_name in hyperparameters:\n",
    "    print(f\"Hyperparameter: {param_name}\")\n",
    "    print(f\"Values: {hyperparameters[param_name]}\")\n",
    "    print(f\"Train F1 Scores: {results[param_name]['train_f1_scores']}\")\n",
    "    print(f\"Test F1 Scores: {results[param_name]['test_f1_scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e284985e",
   "metadata": {},
   "source": [
    "#### Optimisation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81bde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# List of hyperparameters and their ranges\n",
    "hyperparameters = {\n",
    "    'num_leaves': [31, 40, 50, 63, 70, 80, 90, 100, 110, 120, 130],\n",
    "    'n_estimators': [75, 85, 95, 105, 115, 125],\n",
    "    'max_depth': [-1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'min_child_samples': [1, 2, 4, 6, 8, 10],\n",
    "    'reg_lambda': [1, 1.2, 1.4, 1.6, 1.8, 2.0]\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {param: {'train_f1_scores': [], 'test_f1_scores': []} for param in hyperparameters}\n",
    "\n",
    "def evaluate_hyperparameter(param_name, param_values):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"Evaluating {param_name} with value {value}...\")\n",
    "        train_f1_scores = []\n",
    "        test_f1_scores = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 63,  # Default\n",
    "                'n_estimators': 100,  # Default\n",
    "                'max_depth': -1,  # Default\n",
    "                'min_child_samples': 10,  # Default\n",
    "                'subsample': 1.0,  # Default\n",
    "                'colsample_bytree': 1.0,  # Default\n",
    "                'scale_pos_weight': 1.5,  # DONE\n",
    "                'reg_alpha': 1.2,  # DONE\n",
    "                'reg_lambda': 0,  # Default\n",
    "                'random_state': 123\n",
    "            }\n",
    "            params[param_name] = value\n",
    "            \n",
    "            model = LGBMClassifier(**params)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_val_pred = model.predict(X_val_fold)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                train_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "                test_f1_scores.append(f1_score(y_test, y_test_pred, average='macro'))\n",
    "            except Exception as e:\n",
    "                print(f\"Error while training or predicting with {param_name}={value}: {e}\")\n",
    "\n",
    "        if train_f1_scores and test_f1_scores:\n",
    "            results[param_name]['train_f1_scores'].append(np.mean(train_f1_scores))\n",
    "            results[param_name]['test_f1_scores'].append(np.mean(test_f1_scores))\n",
    "        else:\n",
    "            print(f\"No scores collected for {param_name} with value {value}\")\n",
    "\n",
    "# Evaluate each hyperparameter\n",
    "for param_name, param_values in hyperparameters.items():\n",
    "    evaluate_hyperparameter(param_name, param_values)\n",
    "\n",
    "# Plotting results for each hyperparameter\n",
    "for param_name in hyperparameters:\n",
    "    if results[param_name]['train_f1_scores']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['train_f1_scores'], marker='o', label='Train F1 Score')\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['test_f1_scores'], marker='o', label='Test F1 Score')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Macro F1 Score vs. {param_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No results for hyperparameter {param_name}\")\n",
    "\n",
    "# Print results\n",
    "for param_name in hyperparameters:\n",
    "    print(f\"Hyperparameter: {param_name}\")\n",
    "    print(f\"Values: {hyperparameters[param_name]}\")\n",
    "    print(f\"Train F1 Scores: {results[param_name]['train_f1_scores']}\")\n",
    "    print(f\"Test F1 Scores: {results[param_name]['test_f1_scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b7446",
   "metadata": {},
   "source": [
    "#### Optimisation 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e910ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# List of hyperparameters and their ranges\n",
    "hyperparameters = {\n",
    "    'min_child_samples': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'colsample_bytree': [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1],\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {param: {'train_f1_scores': [], 'test_f1_scores': []} for param in hyperparameters}\n",
    "\n",
    "def evaluate_hyperparameter(param_name, param_values):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"Evaluating {param_name} with value {value}...\")\n",
    "        train_f1_scores = []\n",
    "        test_f1_scores = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 70,\n",
    "                'n_estimators': 85,\n",
    "                'max_depth': 30,\n",
    "                'min_child_samples': 2,\n",
    "                'subsample': 1.0,  # Default\n",
    "                'colsample_bytree': 1.0,  # Default\n",
    "                'scale_pos_weight': 1.5,\n",
    "                'reg_alpha': 1.2,\n",
    "                'reg_lambda': 1,\n",
    "                'random_state': 123\n",
    "            }\n",
    "            params[param_name] = value\n",
    "            \n",
    "            model = LGBMClassifier(**params)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_val_pred = model.predict(X_val_fold)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                train_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "                test_f1_scores.append(f1_score(y_test, y_test_pred, average='macro'))\n",
    "                \n",
    "                # Check the impact on training\n",
    "                print(f\"Trained with {param_name}={value} in {time.time() - start_time:.2f} seconds\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while training or predicting with {param_name}={value}: {e}\")\n",
    "\n",
    "        if train_f1_scores and test_f1_scores:\n",
    "            results[param_name]['train_f1_scores'].append(np.mean(train_f1_scores))\n",
    "            results[param_name]['test_f1_scores'].append(np.mean(test_f1_scores))\n",
    "        else:\n",
    "            print(f\"No scores collected for {param_name} with value {value}\")\n",
    "\n",
    "# Evaluate each hyperparameter\n",
    "for param_name, param_values in hyperparameters.items():\n",
    "    evaluate_hyperparameter(param_name, param_values)\n",
    "\n",
    "# Plotting results for each hyperparameter\n",
    "for param_name in hyperparameters:\n",
    "    if results[param_name]['train_f1_scores']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['train_f1_scores'], marker='o', label='Train F1 Score')\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['test_f1_scores'], marker='o', label='Test F1 Score')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Macro F1 Score vs. {param_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No results for hyperparameter {param_name}\")\n",
    "\n",
    "# Print results\n",
    "for param_name in hyperparameters:\n",
    "    print(f\"Hyperparameter: {param_name}\")\n",
    "    print(f\"Values: {hyperparameters[param_name]}\")\n",
    "    print(f\"Train F1 Scores: {results[param_name]['train_f1_scores']}\")\n",
    "    print(f\"Test F1 Scores: {results[param_name]['test_f1_scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cd1b3",
   "metadata": {},
   "source": [
    "#### Optimisation 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# List of hyperparameters and their ranges\n",
    "hyperparameters = {\n",
    "    'colsample_bytree': [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1],\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {param: {'train_f1_scores': [], 'test_f1_scores': []} for param in hyperparameters}\n",
    "\n",
    "def evaluate_hyperparameter(param_name, param_values):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    for value in param_values:\n",
    "        print(f\"Evaluating {param_name} with value {value}...\")\n",
    "        train_f1_scores = []\n",
    "        test_f1_scores = []\n",
    "\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 70,\n",
    "                'n_estimators': 85,\n",
    "                'max_depth': 30,\n",
    "                'min_child_samples': 4,\n",
    "                'subsample': 1.0,  # Default\n",
    "                'colsample_bytree': 1.0,  # Default\n",
    "                'scale_pos_weight': 1.5,\n",
    "                'reg_alpha': 1.2,\n",
    "                'reg_lambda': 1,\n",
    "                'random_state': 123\n",
    "            }\n",
    "            params[param_name] = value\n",
    "            \n",
    "            model = LGBMClassifier(**params)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                y_val_pred = model.predict(X_val_fold)\n",
    "                y_test_pred = model.predict(X_test)\n",
    "                \n",
    "                train_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "                test_f1_scores.append(f1_score(y_test, y_test_pred, average='macro'))\n",
    "                \n",
    "                # Check the impact on training\n",
    "                print(f\"Trained with {param_name}={value} in {time.time() - start_time:.2f} seconds\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error while training or predicting with {param_name}={value}: {e}\")\n",
    "\n",
    "        if train_f1_scores and test_f1_scores:\n",
    "            results[param_name]['train_f1_scores'].append(np.mean(train_f1_scores))\n",
    "            results[param_name]['test_f1_scores'].append(np.mean(test_f1_scores))\n",
    "        else:\n",
    "            print(f\"No scores collected for {param_name} with value {value}\")\n",
    "\n",
    "# Evaluate each hyperparameter\n",
    "for param_name, param_values in hyperparameters.items():\n",
    "    evaluate_hyperparameter(param_name, param_values)\n",
    "\n",
    "# Plotting results for each hyperparameter\n",
    "for param_name in hyperparameters:\n",
    "    if results[param_name]['train_f1_scores']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['train_f1_scores'], marker='o', label='Train F1 Score')\n",
    "        plt.plot(hyperparameters[param_name], results[param_name]['test_f1_scores'], marker='o', label='Test F1 Score')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title(f'Macro F1 Score vs. {param_name}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No results for hyperparameter {param_name}\")\n",
    "\n",
    "# Print results\n",
    "for param_name in hyperparameters:\n",
    "    print(f\"Hyperparameter: {param_name}\")\n",
    "    print(f\"Values: {hyperparameters[param_name]}\")\n",
    "    print(f\"Train F1 Scores: {results[param_name]['train_f1_scores']}\")\n",
    "    print(f\"Test F1 Scores: {results[param_name]['test_f1_scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0944f746",
   "metadata": {},
   "source": [
    "### Optimised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load your training dataset\n",
    "train_data = pd.read_csv('Data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X_train = train_data.drop(['label', 'id'], axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "# Load your test dataset\n",
    "test_data = pd.read_csv('Data/test_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column\n",
    "X_test = test_data.drop(['id'], axis=1).values\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Define model parameters\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 70,\n",
    "    'n_estimators': 85,\n",
    "    'max_depth': 30,\n",
    "    'min_child_samples': 4,\n",
    "    'subsample': 1.0,  # Default\n",
    "    'colsample_bytree': 0.55,\n",
    "    'scale_pos_weight': 1.5,\n",
    "    'reg_alpha': 1.2,\n",
    "    'reg_lambda': 1,\n",
    "    'random_state': 123\n",
    "}\n",
    "\n",
    "# Train the model on the entire training dataset\n",
    "model = LGBMClassifier(**params)\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Model trained in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Output the predictions\n",
    "output = pd.DataFrame({'id': test_data['id'], 'label': y_test_pred})\n",
    "#output.to_csv('Optimal_boosting.csv', index=False) #uncomment to get file, otherwise file is found in /task3_predictions\n",
    "\n",
    "\n",
    "print(\"Predictions saved to Optimal_boosting.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f155b0",
   "metadata": {},
   "source": [
    "The optimal hyperparameters are:\n",
    "- num_leaves: 70\n",
    "- n_estimators: 85\n",
    "- max_depth:30\n",
    "- min_child_samples: 4\n",
    "- colsample_bytree: 0.55\n",
    "- scale_pos_weight: 1.5\n",
    "- reg_alpha: 1\n",
    "- reg_lambda: 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc8c2e",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "Avengers Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test = pd.read_csv(\"Data/test_tfidf_features.csv\")\n",
    "\n",
    "# Read the first set of predictions\n",
    "y_pred1 = pd.read_csv(\"Task3_predictions/Optimal_LogReg.csv\")\n",
    "y_pred1 = y_pred1['label'].values\n",
    "\n",
    "# Read the second set of predictions\n",
    "y_pred2 = pd.read_csv(\"Task3_predictions/Optimal_gbdt.csv\")\n",
    "y_pred2 = y_pred2['label'].values\n",
    "\n",
    "# Read the third set of predictions\n",
    "y_pred3 = pd.read_csv(\"Task3_predictions/Optimal_forest.csv\")\n",
    "y_pred3 = y_pred3['label'].values\n",
    "\n",
    "# Read the fourth set of predictions\n",
    "y_pred4 = pd.read_csv(\"Task3_predictions/Optimal_NB.csv\")\n",
    "y_pred4 = y_pred4['label'].values\n",
    "\n",
    "# Read the fifth set of predictions\n",
    "y_pred5 = pd.read_csv(\"Task3_predictions/Optimal_svm.csv\")\n",
    "y_pred5 = y_pred5['label'].values\n",
    "\n",
    "# Combine predictions into a DataFrame for easier manipulation\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'pred1': y_pred1,\n",
    "    'pred2': y_pred2,\n",
    "    'pred3': y_pred3,\n",
    "    'pred4': y_pred4,\n",
    "    'pred5': y_pred5\n",
    "})\n",
    "\n",
    "# Perform majority voting\n",
    "predictions_df['label'] = (predictions_df[['pred1', 'pred2', 'pred3', 'pred4', 'pred5']].sum(axis=1) > 1).astype(int)\n",
    "\n",
    "#Display the first few rows of the final predictions\n",
    "print(predictions_df.head())\n",
    "\n",
    "# Save the final predictions to a CSV file\n",
    "final_df = predictions_df[['id', 'label']]\n",
    "#final_df.to_csv('final_predictions.csv', index=False) #uncomment to get file, otherwise file is found in root folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
