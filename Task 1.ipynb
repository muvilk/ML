{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "472906de",
   "metadata": {},
   "source": [
    "# 50.007 Machine Learning - Summer 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396518a1",
   "metadata": {},
   "source": [
    "## Task 1: Implement Logistics Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb735b3b",
   "metadata": {},
   "source": [
    "First we define the functions we are going to use, namely:\n",
    "\n",
    "sigmoid(z): A function that takes in a Real Number input and returns an output value between 0 and 1.\n",
    "\n",
    "loss(y, y_hat): A loss function that allows us to minimize and determine the optimal parameters. The function takes in the actual labels y and the predicted labels y_hat, and returns the overall training loss.\n",
    "\n",
    "gradients(X, y, y_hat): The Gradient Descent Algorithm to find the optimal values of our parameters. The function takes in the training feature X, actual labels y and the predicted labels y_hat, and returns the partial derivative of the Loss function with respect to weights (w) and bias (db).\n",
    "\n",
    "train(X, y, bs, epochs, lr, tol= 1e-4): The training function for the model. We modified the normal training function and included mini-batch gradient descent and a tolerance level for early stopping. Mini-batch gradient descent was used for better generalisation in the case of extreme data and for improve training speed.\n",
    "\n",
    "predict(X): The prediction function to apply our validation and test sets.\n",
    "\n",
    "accuracy(y_true, y_pred): function to calculate accuracy of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332abcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sigmoid function for logistic regression\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Loss function for logistic regression\n",
    "def loss(y, y_hat):\n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "# Gradient descent function to compute gradients\n",
    "def gradient_descent(X, y, y_hat):\n",
    "    m = X.shape[0]\n",
    "    dw = (1 / m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1 / m) * np.sum(y_hat - y)\n",
    "    return dw, db\n",
    "\n",
    "# Training function for logistic regression using mini-batch gradient descent\n",
    "def train(X, y, bs, epochs, lr, tol=1e-4):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros((n_features, 1))  # Initialize weights\n",
    "    b = 0  # Initialize bias\n",
    "    y = y.reshape(n_samples, 1)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the dataset\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        # Mini-batch gradient descent\n",
    "        for i in range(0, n_samples, bs):\n",
    "            X_batch = X_shuffled[i:i + bs]\n",
    "            y_batch = y_shuffled[i:i + bs]\n",
    "            \n",
    "            # Compute predictions\n",
    "            y_hat = sigmoid(np.dot(X_batch, w) + b)\n",
    "            # Compute gradients\n",
    "            dw, db = gradient_descent(X_batch, y_batch, y_hat)\n",
    "            \n",
    "            # Update weights and bias\n",
    "            w -= lr * dw\n",
    "            b -= lr * db\n",
    "\n",
    "        # Early Stopping check\n",
    "        if epoch % 100 == 0:\n",
    "            y_hat_full = sigmoid(np.dot(X, w) + b)\n",
    "            current_loss = loss(y, y_hat_full)\n",
    "            print(f'Epoch {epoch}, Loss: {current_loss:.4f}')\n",
    "\n",
    "            if epoch > 10 and abs(previous_loss - current_loss) < tol:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            previous_loss = current_loss\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# Prediction function for logistic regression\n",
    "def predict(X, w, b):\n",
    "    y_hat = sigmoid(np.dot(X, w) + b)\n",
    "    pred = [1 if i > 0.5 else 0 for i in y_hat]\n",
    "    return np.array(pred)\n",
    "\n",
    "# Accuracy calculation function\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca492c",
   "metadata": {},
   "source": [
    "Next we prepare the data. \n",
    "We have also opted to perform a 80-20 split on the training data for to create a training set and validation set respectively. This allows us to measure the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d86a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = pd.read_csv(\"Data/train_tfidf_features.csv\")\n",
    "X_features = data.drop(['label', 'id'], axis=1).values\n",
    "Y_label = data['label'].values\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv(\"Data/test_tfidf_features.csv\")\n",
    "X_test = test.drop(['id'], axis=1).values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "indices = np.arange(X_features.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "split_idx = int(X_features.shape[0] * 0.8)\n",
    "train_indices = indices[:split_idx]\n",
    "val_indices = indices[split_idx:]\n",
    "\n",
    "X_train_split = X_features[train_indices]\n",
    "y_train_split = Y_label[train_indices]\n",
    "X_val_split = X_features[val_indices]\n",
    "y_val_split = Y_label[val_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdee819",
   "metadata": {},
   "source": [
    "Now we can train the model.\n",
    "\n",
    "A large epoch value was used to ensure we converge to a global maximum.\n",
    "\n",
    "A small learning rate was used to ensure smooth convergence and to avoid overshooting.\n",
    "\n",
    "While both of these combined can result in a long run time, the implementation of a tolerance level allows us to stop the training if there is too small a convergence, reducing the run time.\n",
    "\n",
    "The predictions of the model can be found in Data/LogRed_Predictions.csv\n",
    "\n",
    "Note that running the model will still take some time. ~10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb78808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6746\n",
      "Epoch 100, Loss: 0.6409\n",
      "Epoch 200, Loss: 0.6226\n",
      "Epoch 300, Loss: 0.6080\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19852\\64658240.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Predict on the validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19852\\2885699714.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, y, bs, epochs, lr, tol)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mX_shuffled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0my_shuffled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "w, b = train(X_train_split, y_train_split, batch_size, epochs, learning_rate)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = predict(X_val_split, w, b)\n",
    "\n",
    "# Calculate validation accuracy\n",
    "acc = accuracy(y_val_split, y_pred)\n",
    "print(f'Validation Accuracy: {acc * 100:.2f}%')\n",
    "\n",
    "# Predict on the test set\n",
    "y_final = predict(X_test, w, b)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df = pd.DataFrame({'id': test['id'], 'label': y_final})\n",
    "predictions_df.to_csv('LogRed_Prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5777d06",
   "metadata": {},
   "source": [
    "We then run sklearn's logreg package and see how our model compares to it. We will be evaluating based on the accuracy of the 2 models. \n",
    "\n",
    "Our model has an accuracy of 69.86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb51d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = pd.read_csv(\"Data/train_tfidf_features.csv\")\n",
    "X_features = data.drop(['label', 'id'], axis=1).values\n",
    "Y_label = data['label'].values\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv(\"Data/test_tfidf_features.csv\")\n",
    "X_test = test.drop(['id'], axis=1).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_features, Y_label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of the LogisticRegression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d1c578",
   "metadata": {},
   "source": [
    "SKLearn's logreg package has an accuracy of 71.5740471341286%, which is slightly better than our model, but still relatively competitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff25eb",
   "metadata": {},
   "source": [
    "Now, we use 100% of the training set to train the model and make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa232b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = pd.read_csv(\"Data/train_tfidf_features.csv\")\n",
    "X_features = data.drop(['label', 'id'], axis=1).values\n",
    "Y_label = data['label'].values\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv(\"Data/test_tfidf_features.csv\")\n",
    "X_test = test.drop(['id'], axis=1).values\n",
    "\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "w, b = train(X_features, Y_label, batch_size, epochs, learning_rate)\n",
    "\n",
    "# Predict on the test set\n",
    "y_final = predict(X_test, w, b)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df = pd.DataFrame({'id': test['id'], 'label': y_final})\n",
    "predictions_df.to_csv('LogReg_Prediction_SKLearn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
