{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 42, learning rate 0.01, n_estimators 100: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 52, learning rate 0.01, n_estimators 100: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 62, learning rate 0.01, n_estimators 100: 0.63\n",
      "Average Test Accuracy for learning rate 0.01 and n_estimators 100: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 42, learning rate 0.1, n_estimators 100: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 52, learning rate 0.1, n_estimators 100: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 62, learning rate 0.1, n_estimators 100: 0.64\n",
      "Average Test Accuracy for learning rate 0.1 and n_estimators 100: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 42, learning rate 0.5, n_estimators 100: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 52, learning rate 0.5, n_estimators 100: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 62, learning rate 0.5, n_estimators 100: 0.66\n",
      "Average Test Accuracy for learning rate 0.5 and n_estimators 100: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 42, learning rate 1.0, n_estimators 100: 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 52, learning rate 1.0, n_estimators 100: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_79836/1399708117.py:61: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for seed 62, learning rate 1.0, n_estimators 100: 0.68\n",
      "Average Test Accuracy for learning rate 1.0 and n_estimators 100: 0.68\n",
      "Best Accuracy: 0.68\n",
      "Best Learning Rate: 1.0\n",
      "Best Number of Estimators: 100\n",
      "Best Seed: 62\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values  # Convert to NumPy array\n",
    "y = data['label'].values  # Convert to NumPy array\n",
    "\n",
    "# List of random seeds, learning rates, and number of estimators\n",
    "random_seeds = [42, 52, 62]\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
    "\n",
    "# Initialize variables to track the best configuration\n",
    "best_accuracy = 0\n",
    "best_lr = None\n",
    "best_n_estimators = None\n",
    "best_seed = None\n",
    "\n",
    "# Iterate over each combination of learning rate and number of estimators\n",
    "for lr in learning_rates:\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for seed in random_seeds:\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "        # Initialize the base estimator\n",
    "        base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "        # Initialize the AdaBoost classifier\n",
    "        adaboost = AdaBoostClassifier(estimator=base_estimator, n_estimators=10, learning_rate=lr, random_state=seed)\n",
    "\n",
    "        # Define the k-fold cross-validator\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "        # Prepare an array to collect predictions\n",
    "        all_predictions = np.zeros((X_train.shape[0], kf.get_n_splits()), dtype=int)\n",
    "\n",
    "        # Perform cross-validation\n",
    "        for fold_index, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]\n",
    "            \n",
    "            # Train the classifier\n",
    "            adaboost.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # Predict on the validation set\n",
    "            y_pred = adaboost.predict(X_val_fold)\n",
    "            \n",
    "            # Store predictions in the appropriate rows\n",
    "            all_predictions[test_index, fold_index] = y_pred\n",
    "\n",
    "        # Calculate majority vote for each sample\n",
    "        majority_vote_predictions_train = mode(all_predictions, axis=1).mode.flatten()\n",
    "\n",
    "        # Train the classifier on the entire training set\n",
    "        adaboost.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_test_pred = adaboost.predict(X_test)\n",
    "\n",
    "        # Calculate the accuracy on the test set\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        print(f\"Test Accuracy for seed {seed}, learning rate {lr}, n_estimators {n_estimators}: {test_accuracy:.2f}\")\n",
    "\n",
    "    # Calculate and print the average accuracy for this learning rate and number of estimators\n",
    "    average_accuracy = np.mean(test_accuracies)\n",
    "    print(f\"Average Test Accuracy for learning rate {lr} and n_estimators {n_estimators}: {average_accuracy:.2f}\")\n",
    "\n",
    "    # Update best parameters if current average accuracy is better\n",
    "    if average_accuracy > best_accuracy:\n",
    "        best_accuracy = average_accuracy\n",
    "        best_lr = lr\n",
    "        best_n_estimators = n_estimators\n",
    "        best_seed = seed\n",
    "\n",
    "# Print the best configuration\n",
    "print(f\"Best Accuracy: {best_accuracy:.2f}\")\n",
    "print(f\"Best Learning Rate: {best_lr}\")\n",
    "print(f\"Best Number of Estimators: {best_n_estimators}\")\n",
    "print(f\"Best Seed: {best_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with Logistic Regression: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_test_pred_logistic = logistic_model.predict(X_test)\n",
    "test_accuracy_logistic = (y_test_pred_logistic == y_test).mean()\n",
    "print(f\"Test Accuracy with Logistic Regression: {test_accuracy_logistic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4203, number of negative: 6794\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18899\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382195 -> initscore=-0.480241\n",
      "[LightGBM] [Info] Start training from score -0.480241\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6792\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18842\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382377 -> initscore=-0.479471\n",
      "[LightGBM] [Info] Start training from score -0.479471\n",
      "[LightGBM] [Info] Number of positive: 4248, number of negative: 6750\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19019\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.386252 -> initscore=-0.463094\n",
      "[LightGBM] [Info] Start training from score -0.463094\n",
      "[LightGBM] [Info] Number of positive: 4227, number of negative: 6771\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015709 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18998\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 819\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384343 -> initscore=-0.471156\n",
      "[LightGBM] [Info] Start training from score -0.471156\n",
      "[LightGBM] [Info] Number of positive: 4185, number of negative: 6813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19003\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 817\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380524 -> initscore=-0.487326\n",
      "[LightGBM] [Info] Start training from score -0.487326\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24778\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1009\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 4176, number of negative: 6821\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18926\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379740 -> initscore=-0.490652\n",
      "[LightGBM] [Info] Start training from score -0.490652\n",
      "[LightGBM] [Info] Number of positive: 4182, number of negative: 6815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18880\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 808\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380286 -> initscore=-0.488336\n",
      "[LightGBM] [Info] Start training from score -0.488336\n",
      "[LightGBM] [Info] Number of positive: 4166, number of negative: 6832\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18929\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 813\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.378796 -> initscore=-0.494661\n",
      "[LightGBM] [Info] Start training from score -0.494661\n",
      "[LightGBM] [Info] Number of positive: 4170, number of negative: 6828\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18982\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379160 -> initscore=-0.493116\n",
      "[LightGBM] [Info] Start training from score -0.493116\n",
      "[LightGBM] [Info] Number of positive: 4190, number of negative: 6808\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011499 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19001\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 811\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380978 -> initscore=-0.485398\n",
      "[LightGBM] [Info] Start training from score -0.485398\n",
      "[LightGBM] [Info] Number of positive: 5221, number of negative: 8526\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24702\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 996\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379792 -> initscore=-0.490431\n",
      "[LightGBM] [Info] Start training from score -0.490431\n",
      "[LightGBM] [Info] Number of positive: 4199, number of negative: 6798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18868\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381831 -> initscore=-0.481782\n",
      "[LightGBM] [Info] Start training from score -0.481782\n",
      "[LightGBM] [Info] Number of positive: 4132, number of negative: 6865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18726\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 799\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375739 -> initscore=-0.507674\n",
      "[LightGBM] [Info] Start training from score -0.507674\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18838\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382342 -> initscore=-0.479618\n",
      "[LightGBM] [Info] Start training from score -0.479618\n",
      "[LightGBM] [Info] Number of positive: 4179, number of negative: 6819\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18823\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379978 -> initscore=-0.489641\n",
      "[LightGBM] [Info] Start training from score -0.489641\n",
      "[LightGBM] [Info] Number of positive: 4153, number of negative: 6845\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011345 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18922\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377614 -> initscore=-0.499687\n",
      "[LightGBM] [Info] Start training from score -0.499687\n",
      "[LightGBM] [Info] Number of positive: 5217, number of negative: 8530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24607\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379501 -> initscore=-0.491667\n",
      "[LightGBM] [Info] Start training from score -0.491667\n",
      "[LightGBM] [Info] Number of positive: 4203, number of negative: 6794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18899\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382195 -> initscore=-0.480241\n",
      "[LightGBM] [Info] Start training from score -0.480241\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6792\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18842\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382377 -> initscore=-0.479471\n",
      "[LightGBM] [Info] Start training from score -0.479471\n",
      "[LightGBM] [Info] Number of positive: 4248, number of negative: 6750\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19019\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.386252 -> initscore=-0.463094\n",
      "[LightGBM] [Info] Start training from score -0.463094\n",
      "[LightGBM] [Info] Number of positive: 4227, number of negative: 6771\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18998\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 819\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384343 -> initscore=-0.471156\n",
      "[LightGBM] [Info] Start training from score -0.471156\n",
      "[LightGBM] [Info] Number of positive: 4185, number of negative: 6813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19003\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 817\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380524 -> initscore=-0.487326\n",
      "[LightGBM] [Info] Start training from score -0.487326\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24778\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1009\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 4176, number of negative: 6821\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18926\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379740 -> initscore=-0.490652\n",
      "[LightGBM] [Info] Start training from score -0.490652\n",
      "[LightGBM] [Info] Number of positive: 4182, number of negative: 6815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18880\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 808\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380286 -> initscore=-0.488336\n",
      "[LightGBM] [Info] Start training from score -0.488336\n",
      "[LightGBM] [Info] Number of positive: 4166, number of negative: 6832\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024578 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18929\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 813\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.378796 -> initscore=-0.494661\n",
      "[LightGBM] [Info] Start training from score -0.494661\n",
      "[LightGBM] [Info] Number of positive: 4170, number of negative: 6828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18982\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379160 -> initscore=-0.493116\n",
      "[LightGBM] [Info] Start training from score -0.493116\n",
      "[LightGBM] [Info] Number of positive: 4190, number of negative: 6808\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19001\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 811\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380978 -> initscore=-0.485398\n",
      "[LightGBM] [Info] Start training from score -0.485398\n",
      "[LightGBM] [Info] Number of positive: 5221, number of negative: 8526\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24702\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 996\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379792 -> initscore=-0.490431\n",
      "[LightGBM] [Info] Start training from score -0.490431\n",
      "[LightGBM] [Info] Number of positive: 4199, number of negative: 6798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18868\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381831 -> initscore=-0.481782\n",
      "[LightGBM] [Info] Start training from score -0.481782\n",
      "[LightGBM] [Info] Number of positive: 4132, number of negative: 6865\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18726\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 799\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375739 -> initscore=-0.507674\n",
      "[LightGBM] [Info] Start training from score -0.507674\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18838\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382342 -> initscore=-0.479618\n",
      "[LightGBM] [Info] Start training from score -0.479618\n",
      "[LightGBM] [Info] Number of positive: 4179, number of negative: 6819\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18823\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379978 -> initscore=-0.489641\n",
      "[LightGBM] [Info] Start training from score -0.489641\n",
      "[LightGBM] [Info] Number of positive: 4153, number of negative: 6845\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18922\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377614 -> initscore=-0.499687\n",
      "[LightGBM] [Info] Start training from score -0.499687\n",
      "[LightGBM] [Info] Number of positive: 5217, number of negative: 8530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24607\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379501 -> initscore=-0.491667\n",
      "[LightGBM] [Info] Start training from score -0.491667\n",
      "[LightGBM] [Info] Number of positive: 4203, number of negative: 6794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18899\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382195 -> initscore=-0.480241\n",
      "[LightGBM] [Info] Start training from score -0.480241\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6792\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18842\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382377 -> initscore=-0.479471\n",
      "[LightGBM] [Info] Start training from score -0.479471\n",
      "[LightGBM] [Info] Number of positive: 4248, number of negative: 6750\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027127 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19019\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.386252 -> initscore=-0.463094\n",
      "[LightGBM] [Info] Start training from score -0.463094\n",
      "[LightGBM] [Info] Number of positive: 4227, number of negative: 6771\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18998\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 819\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384343 -> initscore=-0.471156\n",
      "[LightGBM] [Info] Start training from score -0.471156\n",
      "[LightGBM] [Info] Number of positive: 4185, number of negative: 6813\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19003\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 817\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380524 -> initscore=-0.487326\n",
      "[LightGBM] [Info] Start training from score -0.487326\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24778\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1009\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 4176, number of negative: 6821\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18926\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379740 -> initscore=-0.490652\n",
      "[LightGBM] [Info] Start training from score -0.490652\n",
      "[LightGBM] [Info] Number of positive: 4182, number of negative: 6815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18880\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 808\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380286 -> initscore=-0.488336\n",
      "[LightGBM] [Info] Start training from score -0.488336\n",
      "[LightGBM] [Info] Number of positive: 4166, number of negative: 6832\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18929\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 813\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.378796 -> initscore=-0.494661\n",
      "[LightGBM] [Info] Start training from score -0.494661\n",
      "[LightGBM] [Info] Number of positive: 4170, number of negative: 6828\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18982\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379160 -> initscore=-0.493116\n",
      "[LightGBM] [Info] Start training from score -0.493116\n",
      "[LightGBM] [Info] Number of positive: 4190, number of negative: 6808\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19001\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 811\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380978 -> initscore=-0.485398\n",
      "[LightGBM] [Info] Start training from score -0.485398\n",
      "[LightGBM] [Info] Number of positive: 5221, number of negative: 8526\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24702\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 996\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379792 -> initscore=-0.490431\n",
      "[LightGBM] [Info] Start training from score -0.490431\n",
      "[LightGBM] [Info] Number of positive: 4199, number of negative: 6798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18868\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381831 -> initscore=-0.481782\n",
      "[LightGBM] [Info] Start training from score -0.481782\n",
      "[LightGBM] [Info] Number of positive: 4132, number of negative: 6865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18726\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 799\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375739 -> initscore=-0.507674\n",
      "[LightGBM] [Info] Start training from score -0.507674\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18838\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382342 -> initscore=-0.479618\n",
      "[LightGBM] [Info] Start training from score -0.479618\n",
      "[LightGBM] [Info] Number of positive: 4179, number of negative: 6819\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18823\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379978 -> initscore=-0.489641\n",
      "[LightGBM] [Info] Start training from score -0.489641\n",
      "[LightGBM] [Info] Number of positive: 4153, number of negative: 6845\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18922\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377614 -> initscore=-0.499687\n",
      "[LightGBM] [Info] Start training from score -0.499687\n",
      "[LightGBM] [Info] Number of positive: 5217, number of negative: 8530\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24607\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379501 -> initscore=-0.491667\n",
      "[LightGBM] [Info] Start training from score -0.491667\n",
      "[LightGBM] [Info] Number of positive: 4203, number of negative: 6794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18899\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382195 -> initscore=-0.480241\n",
      "[LightGBM] [Info] Start training from score -0.480241\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6792\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18842\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382377 -> initscore=-0.479471\n",
      "[LightGBM] [Info] Start training from score -0.479471\n",
      "[LightGBM] [Info] Number of positive: 4248, number of negative: 6750\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19019\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.386252 -> initscore=-0.463094\n",
      "[LightGBM] [Info] Start training from score -0.463094\n",
      "[LightGBM] [Info] Number of positive: 4227, number of negative: 6771\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18998\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 819\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384343 -> initscore=-0.471156\n",
      "[LightGBM] [Info] Start training from score -0.471156\n",
      "[LightGBM] [Info] Number of positive: 4185, number of negative: 6813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19003\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 817\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380524 -> initscore=-0.487326\n",
      "[LightGBM] [Info] Start training from score -0.487326\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031578 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24778\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1009\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 4176, number of negative: 6821\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012227 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18926\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379740 -> initscore=-0.490652\n",
      "[LightGBM] [Info] Start training from score -0.490652\n",
      "[LightGBM] [Info] Number of positive: 4182, number of negative: 6815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18880\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 808\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380286 -> initscore=-0.488336\n",
      "[LightGBM] [Info] Start training from score -0.488336\n",
      "[LightGBM] [Info] Number of positive: 4166, number of negative: 6832\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18929\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 813\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.378796 -> initscore=-0.494661\n",
      "[LightGBM] [Info] Start training from score -0.494661\n",
      "[LightGBM] [Info] Number of positive: 4170, number of negative: 6828\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18982\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379160 -> initscore=-0.493116\n",
      "[LightGBM] [Info] Start training from score -0.493116\n",
      "[LightGBM] [Info] Number of positive: 4190, number of negative: 6808\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19001\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 811\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380978 -> initscore=-0.485398\n",
      "[LightGBM] [Info] Start training from score -0.485398\n",
      "[LightGBM] [Info] Number of positive: 5221, number of negative: 8526\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24702\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 996\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379792 -> initscore=-0.490431\n",
      "[LightGBM] [Info] Start training from score -0.490431\n",
      "[LightGBM] [Info] Number of positive: 4199, number of negative: 6798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18868\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381831 -> initscore=-0.481782\n",
      "[LightGBM] [Info] Start training from score -0.481782\n",
      "[LightGBM] [Info] Number of positive: 4132, number of negative: 6865\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18726\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 799\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375739 -> initscore=-0.507674\n",
      "[LightGBM] [Info] Start training from score -0.507674\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6793\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18838\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382342 -> initscore=-0.479618\n",
      "[LightGBM] [Info] Start training from score -0.479618\n",
      "[LightGBM] [Info] Number of positive: 4179, number of negative: 6819\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023707 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18823\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379978 -> initscore=-0.489641\n",
      "[LightGBM] [Info] Start training from score -0.489641\n",
      "[LightGBM] [Info] Number of positive: 4153, number of negative: 6845\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18922\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377614 -> initscore=-0.499687\n",
      "[LightGBM] [Info] Start training from score -0.499687\n",
      "[LightGBM] [Info] Number of positive: 5217, number of negative: 8530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24607\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379501 -> initscore=-0.491667\n",
      "[LightGBM] [Info] Start training from score -0.491667\n",
      "[LightGBM] [Info] Number of positive: 4203, number of negative: 6794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18899\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382195 -> initscore=-0.480241\n",
      "[LightGBM] [Info] Start training from score -0.480241\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6792\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18842\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382377 -> initscore=-0.479471\n",
      "[LightGBM] [Info] Start training from score -0.479471\n",
      "[LightGBM] [Info] Number of positive: 4248, number of negative: 6750\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018224 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19019\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.386252 -> initscore=-0.463094\n",
      "[LightGBM] [Info] Start training from score -0.463094\n",
      "[LightGBM] [Info] Number of positive: 4227, number of negative: 6771\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18998\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 819\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384343 -> initscore=-0.471156\n",
      "[LightGBM] [Info] Start training from score -0.471156\n",
      "[LightGBM] [Info] Number of positive: 4185, number of negative: 6813\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19003\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 817\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380524 -> initscore=-0.487326\n",
      "[LightGBM] [Info] Start training from score -0.487326\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24778\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1009\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 4176, number of negative: 6821\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18926\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379740 -> initscore=-0.490652\n",
      "[LightGBM] [Info] Start training from score -0.490652\n",
      "[LightGBM] [Info] Number of positive: 4182, number of negative: 6815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18880\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 808\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380286 -> initscore=-0.488336\n",
      "[LightGBM] [Info] Start training from score -0.488336\n",
      "[LightGBM] [Info] Number of positive: 4166, number of negative: 6832\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18929\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 813\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.378796 -> initscore=-0.494661\n",
      "[LightGBM] [Info] Start training from score -0.494661\n",
      "[LightGBM] [Info] Number of positive: 4170, number of negative: 6828\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18982\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379160 -> initscore=-0.493116\n",
      "[LightGBM] [Info] Start training from score -0.493116\n",
      "[LightGBM] [Info] Number of positive: 4190, number of negative: 6808\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19001\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 811\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380978 -> initscore=-0.485398\n",
      "[LightGBM] [Info] Start training from score -0.485398\n",
      "[LightGBM] [Info] Number of positive: 5221, number of negative: 8526\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24702\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 996\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379792 -> initscore=-0.490431\n",
      "[LightGBM] [Info] Start training from score -0.490431\n",
      "[LightGBM] [Info] Number of positive: 4199, number of negative: 6798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18868\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381831 -> initscore=-0.481782\n",
      "[LightGBM] [Info] Start training from score -0.481782\n",
      "[LightGBM] [Info] Number of positive: 4132, number of negative: 6865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18726\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 799\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375739 -> initscore=-0.507674\n",
      "[LightGBM] [Info] Start training from score -0.507674\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6793\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18838\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382342 -> initscore=-0.479618\n",
      "[LightGBM] [Info] Start training from score -0.479618\n",
      "[LightGBM] [Info] Number of positive: 4179, number of negative: 6819\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021170 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18823\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379978 -> initscore=-0.489641\n",
      "[LightGBM] [Info] Start training from score -0.489641\n",
      "[LightGBM] [Info] Number of positive: 4153, number of negative: 6845\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18922\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377614 -> initscore=-0.499687\n",
      "[LightGBM] [Info] Start training from score -0.499687\n",
      "[LightGBM] [Info] Number of positive: 5217, number of negative: 8530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24607\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379501 -> initscore=-0.491667\n",
      "[LightGBM] [Info] Start training from score -0.491667\n",
      "[LightGBM] [Info] Number of positive: 4203, number of negative: 6794\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18899\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382195 -> initscore=-0.480241\n",
      "[LightGBM] [Info] Start training from score -0.480241\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6792\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18842\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382377 -> initscore=-0.479471\n",
      "[LightGBM] [Info] Start training from score -0.479471\n",
      "[LightGBM] [Info] Number of positive: 4248, number of negative: 6750\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19019\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.386252 -> initscore=-0.463094\n",
      "[LightGBM] [Info] Start training from score -0.463094\n",
      "[LightGBM] [Info] Number of positive: 4227, number of negative: 6771\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019548 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18998\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 819\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384343 -> initscore=-0.471156\n",
      "[LightGBM] [Info] Start training from score -0.471156\n",
      "[LightGBM] [Info] Number of positive: 4185, number of negative: 6813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19003\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 817\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380524 -> initscore=-0.487326\n",
      "[LightGBM] [Info] Start training from score -0.487326\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24778\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1009\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 4176, number of negative: 6821\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18926\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379740 -> initscore=-0.490652\n",
      "[LightGBM] [Info] Start training from score -0.490652\n",
      "[LightGBM] [Info] Number of positive: 4182, number of negative: 6815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18880\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 808\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380286 -> initscore=-0.488336\n",
      "[LightGBM] [Info] Start training from score -0.488336\n",
      "[LightGBM] [Info] Number of positive: 4166, number of negative: 6832\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18929\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 813\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.378796 -> initscore=-0.494661\n",
      "[LightGBM] [Info] Start training from score -0.494661\n",
      "[LightGBM] [Info] Number of positive: 4170, number of negative: 6828\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016829 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18982\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379160 -> initscore=-0.493116\n",
      "[LightGBM] [Info] Start training from score -0.493116\n",
      "[LightGBM] [Info] Number of positive: 4190, number of negative: 6808\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19001\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 811\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380978 -> initscore=-0.485398\n",
      "[LightGBM] [Info] Start training from score -0.485398\n",
      "[LightGBM] [Info] Number of positive: 5221, number of negative: 8526\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24702\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 996\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379792 -> initscore=-0.490431\n",
      "[LightGBM] [Info] Start training from score -0.490431\n",
      "[LightGBM] [Info] Number of positive: 4199, number of negative: 6798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18868\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381831 -> initscore=-0.481782\n",
      "[LightGBM] [Info] Start training from score -0.481782\n",
      "[LightGBM] [Info] Number of positive: 4132, number of negative: 6865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18726\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 799\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375739 -> initscore=-0.507674\n",
      "[LightGBM] [Info] Start training from score -0.507674\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18838\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382342 -> initscore=-0.479618\n",
      "[LightGBM] [Info] Start training from score -0.479618\n",
      "[LightGBM] [Info] Number of positive: 4179, number of negative: 6819\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027585 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18823\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379978 -> initscore=-0.489641\n",
      "[LightGBM] [Info] Start training from score -0.489641\n",
      "[LightGBM] [Info] Number of positive: 4153, number of negative: 6845\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18922\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377614 -> initscore=-0.499687\n",
      "[LightGBM] [Info] Start training from score -0.499687\n",
      "[LightGBM] [Info] Number of positive: 5217, number of negative: 8530\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24607\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379501 -> initscore=-0.491667\n",
      "[LightGBM] [Info] Start training from score -0.491667\n",
      "[LightGBM] [Info] Number of positive: 4203, number of negative: 6794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18899\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382195 -> initscore=-0.480241\n",
      "[LightGBM] [Info] Start training from score -0.480241\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6792\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18842\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382377 -> initscore=-0.479471\n",
      "[LightGBM] [Info] Start training from score -0.479471\n",
      "[LightGBM] [Info] Number of positive: 4248, number of negative: 6750\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19019\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.386252 -> initscore=-0.463094\n",
      "[LightGBM] [Info] Start training from score -0.463094\n",
      "[LightGBM] [Info] Number of positive: 4227, number of negative: 6771\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18998\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 819\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384343 -> initscore=-0.471156\n",
      "[LightGBM] [Info] Start training from score -0.471156\n",
      "[LightGBM] [Info] Number of positive: 4185, number of negative: 6813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023237 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19003\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 817\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380524 -> initscore=-0.487326\n",
      "[LightGBM] [Info] Start training from score -0.487326\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24778\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1009\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 4176, number of negative: 6821\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18926\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379740 -> initscore=-0.490652\n",
      "[LightGBM] [Info] Start training from score -0.490652\n",
      "[LightGBM] [Info] Number of positive: 4182, number of negative: 6815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18880\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 808\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380286 -> initscore=-0.488336\n",
      "[LightGBM] [Info] Start training from score -0.488336\n",
      "[LightGBM] [Info] Number of positive: 4166, number of negative: 6832\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18929\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 813\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.378796 -> initscore=-0.494661\n",
      "[LightGBM] [Info] Start training from score -0.494661\n",
      "[LightGBM] [Info] Number of positive: 4170, number of negative: 6828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18982\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379160 -> initscore=-0.493116\n",
      "[LightGBM] [Info] Start training from score -0.493116\n",
      "[LightGBM] [Info] Number of positive: 4190, number of negative: 6808\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19001\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 811\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380978 -> initscore=-0.485398\n",
      "[LightGBM] [Info] Start training from score -0.485398\n",
      "[LightGBM] [Info] Number of positive: 5221, number of negative: 8526\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24702\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 996\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379792 -> initscore=-0.490431\n",
      "[LightGBM] [Info] Start training from score -0.490431\n",
      "[LightGBM] [Info] Number of positive: 4199, number of negative: 6798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18868\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381831 -> initscore=-0.481782\n",
      "[LightGBM] [Info] Start training from score -0.481782\n",
      "[LightGBM] [Info] Number of positive: 4132, number of negative: 6865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18726\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 799\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375739 -> initscore=-0.507674\n",
      "[LightGBM] [Info] Start training from score -0.507674\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18838\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382342 -> initscore=-0.479618\n",
      "[LightGBM] [Info] Start training from score -0.479618\n",
      "[LightGBM] [Info] Number of positive: 4179, number of negative: 6819\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18823\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379978 -> initscore=-0.489641\n",
      "[LightGBM] [Info] Start training from score -0.489641\n",
      "[LightGBM] [Info] Number of positive: 4153, number of negative: 6845\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18922\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377614 -> initscore=-0.499687\n",
      "[LightGBM] [Info] Start training from score -0.499687\n",
      "[LightGBM] [Info] Number of positive: 5217, number of negative: 8530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24607\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379501 -> initscore=-0.491667\n",
      "[LightGBM] [Info] Start training from score -0.491667\n",
      "[LightGBM] [Info] Number of positive: 4203, number of negative: 6794\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18899\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382195 -> initscore=-0.480241\n",
      "[LightGBM] [Info] Start training from score -0.480241\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6792\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18842\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382377 -> initscore=-0.479471\n",
      "[LightGBM] [Info] Start training from score -0.479471\n",
      "[LightGBM] [Info] Number of positive: 4248, number of negative: 6750\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19019\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.386252 -> initscore=-0.463094\n",
      "[LightGBM] [Info] Start training from score -0.463094\n",
      "[LightGBM] [Info] Number of positive: 4227, number of negative: 6771\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18998\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 819\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384343 -> initscore=-0.471156\n",
      "[LightGBM] [Info] Start training from score -0.471156\n",
      "[LightGBM] [Info] Number of positive: 4185, number of negative: 6813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19003\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 817\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380524 -> initscore=-0.487326\n",
      "[LightGBM] [Info] Start training from score -0.487326\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020975 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24778\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1009\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 4176, number of negative: 6821\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18926\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379740 -> initscore=-0.490652\n",
      "[LightGBM] [Info] Start training from score -0.490652\n",
      "[LightGBM] [Info] Number of positive: 4182, number of negative: 6815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18880\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 808\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380286 -> initscore=-0.488336\n",
      "[LightGBM] [Info] Start training from score -0.488336\n",
      "[LightGBM] [Info] Number of positive: 4166, number of negative: 6832\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18929\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 813\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.378796 -> initscore=-0.494661\n",
      "[LightGBM] [Info] Start training from score -0.494661\n",
      "[LightGBM] [Info] Number of positive: 4170, number of negative: 6828\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18982\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379160 -> initscore=-0.493116\n",
      "[LightGBM] [Info] Start training from score -0.493116\n",
      "[LightGBM] [Info] Number of positive: 4190, number of negative: 6808\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19001\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 811\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380978 -> initscore=-0.485398\n",
      "[LightGBM] [Info] Start training from score -0.485398\n",
      "[LightGBM] [Info] Number of positive: 5221, number of negative: 8526\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24702\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 996\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379792 -> initscore=-0.490431\n",
      "[LightGBM] [Info] Start training from score -0.490431\n",
      "[LightGBM] [Info] Number of positive: 4199, number of negative: 6798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18868\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 807\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381831 -> initscore=-0.481782\n",
      "[LightGBM] [Info] Start training from score -0.481782\n",
      "[LightGBM] [Info] Number of positive: 4132, number of negative: 6865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18726\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 799\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375739 -> initscore=-0.507674\n",
      "[LightGBM] [Info] Start training from score -0.507674\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18838\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 803\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382342 -> initscore=-0.479618\n",
      "[LightGBM] [Info] Start training from score -0.479618\n",
      "[LightGBM] [Info] Number of positive: 4179, number of negative: 6819\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18823\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 801\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379978 -> initscore=-0.489641\n",
      "[LightGBM] [Info] Start training from score -0.489641\n",
      "[LightGBM] [Info] Number of positive: 4153, number of negative: 6845\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 18922\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 814\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377614 -> initscore=-0.499687\n",
      "[LightGBM] [Info] Start training from score -0.499687\n",
      "[LightGBM] [Info] Number of positive: 5217, number of negative: 8530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24607\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 999\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379501 -> initscore=-0.491667\n",
      "[LightGBM] [Info] Start training from score -0.491667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_81109/2853215664.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom_seeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                             \u001b[0;31m# Split data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                             \u001b[0;31m# Define the k-fold cross-validator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2803\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_common_namespace_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2805\u001b[0;31m     return list(\n\u001b[0m\u001b[1;32m   2806\u001b[0m         chain.from_iterable(\n\u001b[1;32m   2807\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2805\u001b[0m     return list(\n\u001b[1;32m   2806\u001b[0m         chain.from_iterable(\n\u001b[0;32m-> 2807\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2808\u001b[0m         )\n\u001b[1;32m   2809\u001b[0m     )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_indexing.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_polars_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_indexing.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values  # Convert to NumPy array\n",
    "y = data['label'].values  # Convert to NumPy array\n",
    "\n",
    "# List of random seeds, number of estimators, boosting types, number of leaves, max depth, subsample, colsample_bytree\n",
    "random_seeds = [42, 52, 62]\n",
    "n_estimators_list = [50, 100]\n",
    "boosting_types = ['gbdt', 'dart']\n",
    "num_leaves_list = [15, 31, 63]\n",
    "max_depth_list = [-1, 10, 20]  # -1 means no limit\n",
    "subsample_list = [0.8, 0.9, 1.0]\n",
    "colsample_bytree_list = [0.8, 0.9, 1.0]\n",
    "\n",
    "# Initialize variables to track the best configuration\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Store results for all configurations\n",
    "results = []\n",
    "\n",
    "# Iterate over each combination of boosting type, number of estimators, number of leaves, max depth, subsample, and colsample_bytree\n",
    "for boosting_type in boosting_types:\n",
    "    for n_estimators in n_estimators_list:\n",
    "        for num_leaves in num_leaves_list:\n",
    "            for max_depth in max_depth_list:\n",
    "                for subsample in subsample_list:\n",
    "                    for colsample_bytree in colsample_bytree_list:\n",
    "                        test_accuracies = []\n",
    "\n",
    "                        for seed in random_seeds:\n",
    "                            # Split data into training and testing sets\n",
    "                            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "                            # Define the k-fold cross-validator\n",
    "                            kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "                            # Prepare an array to collect predictions\n",
    "                            all_predictions = np.zeros((X_train.shape[0], kf.get_n_splits()), dtype=int)\n",
    "\n",
    "                            # Perform cross-validation\n",
    "                            for fold_index, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "                                X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "                                y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "                                # Initialize the LightGBM classifier\n",
    "                                lgbm = lgb.LGBMClassifier(\n",
    "                                    n_estimators=n_estimators,\n",
    "                                    learning_rate=0.1,\n",
    "                                    boosting_type=boosting_type,\n",
    "                                    num_leaves=num_leaves,\n",
    "                                    max_depth=max_depth,\n",
    "                                    subsample=subsample,\n",
    "                                    colsample_bytree=colsample_bytree,\n",
    "                                    random_state=seed\n",
    "                                )\n",
    "\n",
    "                                # Train the classifier\n",
    "                                lgbm.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "                                # Predict on the validation set\n",
    "                                y_pred = lgbm.predict(X_val_fold)\n",
    "\n",
    "                                # Store predictions in the appropriate rows\n",
    "                                all_predictions[val_index, fold_index] = y_pred\n",
    "\n",
    "                            # Calculate majority vote for each sample\n",
    "                            majority_vote_predictions_train = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=all_predictions)\n",
    "\n",
    "                            # Train the classifier on the entire training set\n",
    "                            lgbm.fit(X_train, y_train)\n",
    "\n",
    "                            # Predict on the test set\n",
    "                            y_test_pred = lgbm.predict(X_test)\n",
    "\n",
    "                            # Calculate the accuracy on the test set\n",
    "                            test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "                            test_accuracies.append(test_accuracy)\n",
    "\n",
    "                        # Calculate and store the average accuracy for this combination of parameters\n",
    "                        average_accuracy = np.mean(test_accuracies)\n",
    "                        results.append((boosting_type, n_estimators, num_leaves, max_depth, subsample, colsample_bytree, average_accuracy))\n",
    "\n",
    "                        # Update best parameters if current average accuracy is better\n",
    "                        if average_accuracy > best_accuracy:\n",
    "                            best_accuracy = average_accuracy\n",
    "                            best_params = {\n",
    "                                'boosting_type': boosting_type,\n",
    "                                'n_estimators': n_estimators,\n",
    "                                'num_leaves': num_leaves,\n",
    "                                'max_depth': max_depth,\n",
    "                                'subsample': subsample,\n",
    "                                'colsample_bytree': colsample_bytree\n",
    "                            }\n",
    "\n",
    "# Print all results at the end\n",
    "print(\"\\nAll Results:\")\n",
    "for boosting_type, n_estimators, num_leaves, max_depth, subsample, colsample_bytree, accuracy in results:\n",
    "    print(f\"Boosting Type: {boosting_type}, Number of Estimators: {n_estimators}, Num Leaves: {num_leaves}, Max Depth: {max_depth}, Subsample: {subsample}, Colsample_bytree: {colsample_bytree}, Average Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print the best configuration\n",
    "print(f\"\\nBest Accuracy: {best_accuracy:.2f}\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
