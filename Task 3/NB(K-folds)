import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score,KFold, cross_val_predict
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
import joblib

#loading the data
data = pd.read_csv('Data/train_tfidf_features.csv')
#print(data.head())  #test
X = data.drop(columns=['id','label'])
Y = data['label']

#Initialise the model: 
model = MultinomialNB()
'''
has 2 main parameters(alpha and fit_prior)
alpha is the smoothing parameter, prevents model from assigning 0 probability to unseen words
adds alpha to count when calculating conditional probability. (default =1 )
fit_prior is a boolean that sets such that model will calculate likelihood and probability of each feature/class from the given training set. (default = True)
Also important to note that SciKit uses log likelihood when calculating 
'''

k = 10
cross_validation_scores = cross_val_score(model, X, Y, cv=k, scoring = 'f1_macro')
kf = KFold(n_splits=k, shuffle=True, random_state=42)
Y_pred = cross_val_predict(model, X, Y, cv=kf)
print("Predictions:")
print(Y_pred)


# Print the F1 scores for each fold
print(f"Macro F1 scores for {k} folds: {cross_validation_scores}")

# Calculate and print the average Macro F1 score
average_score = cross_validation_scores.mean()
print(f"Average Macro F1 score: {average_score}")

#5-folds: Average Macro F1 score: 0.6678869615960173
#10 folds: Average Macro F1 score: 0.6727390293748341