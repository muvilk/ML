{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 307020\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1204\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 5272, number of negative: 8475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 307020\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1204\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383502 -> initscore=-0.474711\n",
      "[LightGBM] [Info] Start training from score -0.474711\n",
      "[LightGBM] [Info] Number of positive: 5191, number of negative: 8556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 307020\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1204\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377610 -> initscore=-0.499706\n",
      "[LightGBM] [Info] Start training from score -0.499706\n",
      "[LightGBM] [Info] Number of positive: 5212, number of negative: 8535\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050189 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 307020\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1204\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379137 -> initscore=-0.493212\n",
      "[LightGBM] [Info] Start training from score -0.493212\n",
      "[LightGBM] [Info] Number of positive: 5262, number of negative: 8486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 307020\n",
      "[LightGBM] [Info] Number of data points in the train set: 13748, number of used features: 1204\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382747 -> initscore=-0.477907\n",
      "[LightGBM] [Info] Start training from score -0.477907\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 373320\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1464\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 5272, number of negative: 8475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 373320\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1464\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383502 -> initscore=-0.474711\n",
      "[LightGBM] [Info] Start training from score -0.474711\n",
      "[LightGBM] [Info] Number of positive: 5191, number of negative: 8556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 373320\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1464\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377610 -> initscore=-0.499706\n",
      "[LightGBM] [Info] Start training from score -0.499706\n",
      "[LightGBM] [Info] Number of positive: 5212, number of negative: 8535\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 373320\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1464\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379137 -> initscore=-0.493212\n",
      "[LightGBM] [Info] Start training from score -0.493212\n",
      "[LightGBM] [Info] Number of positive: 5262, number of negative: 8486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 373320\n",
      "[LightGBM] [Info] Number of data points in the train set: 13748, number of used features: 1464\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382747 -> initscore=-0.477907\n",
      "[LightGBM] [Info] Start training from score -0.477907\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 454665\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1783\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 5272, number of negative: 8475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112178 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 454665\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1783\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383502 -> initscore=-0.474711\n",
      "[LightGBM] [Info] Start training from score -0.474711\n",
      "[LightGBM] [Info] Number of positive: 5191, number of negative: 8556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 454665\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1783\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377610 -> initscore=-0.499706\n",
      "[LightGBM] [Info] Start training from score -0.499706\n",
      "[LightGBM] [Info] Number of positive: 5212, number of negative: 8535\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 454665\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 1783\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379137 -> initscore=-0.493212\n",
      "[LightGBM] [Info] Start training from score -0.493212\n",
      "[LightGBM] [Info] Number of positive: 5262, number of negative: 8486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 454665\n",
      "[LightGBM] [Info] Number of data points in the train set: 13748, number of used features: 1783\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382747 -> initscore=-0.477907\n",
      "[LightGBM] [Info] Start training from score -0.477907\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.163167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 556665\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 2183\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 5272, number of negative: 8475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 556665\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 2183\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383502 -> initscore=-0.474711\n",
      "[LightGBM] [Info] Start training from score -0.474711\n",
      "[LightGBM] [Info] Number of positive: 5191, number of negative: 8556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 556665\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 2183\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377610 -> initscore=-0.499706\n",
      "[LightGBM] [Info] Start training from score -0.499706\n",
      "[LightGBM] [Info] Number of positive: 5212, number of negative: 8535\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 556665\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 2183\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379137 -> initscore=-0.493212\n",
      "[LightGBM] [Info] Start training from score -0.493212\n",
      "[LightGBM] [Info] Number of positive: 5262, number of negative: 8486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 556665\n",
      "[LightGBM] [Info] Number of data points in the train set: 13748, number of used features: 2183\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382747 -> initscore=-0.477907\n",
      "[LightGBM] [Info] Start training from score -0.477907\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 689265\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 2703\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 5272, number of negative: 8475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.206882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 689265\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 2703\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383502 -> initscore=-0.474711\n",
      "[LightGBM] [Info] Start training from score -0.474711\n",
      "[LightGBM] [Info] Number of positive: 5191, number of negative: 8556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 689265\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 2703\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377610 -> initscore=-0.499706\n",
      "[LightGBM] [Info] Start training from score -0.499706\n",
      "[LightGBM] [Info] Number of positive: 5212, number of negative: 8535\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.217219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 689265\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 2703\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379137 -> initscore=-0.493212\n",
      "[LightGBM] [Info] Start training from score -0.493212\n",
      "[LightGBM] [Info] Number of positive: 5262, number of negative: 8486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 689265\n",
      "[LightGBM] [Info] Number of data points in the train set: 13748, number of used features: 2703\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382747 -> initscore=-0.477907\n",
      "[LightGBM] [Info] Start training from score -0.477907\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 5272, number of negative: 8475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.286804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383502 -> initscore=-0.474711\n",
      "[LightGBM] [Info] Start training from score -0.474711\n",
      "[LightGBM] [Info] Number of positive: 5191, number of negative: 8556\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300879 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377610 -> initscore=-0.499706\n",
      "[LightGBM] [Info] Start training from score -0.499706\n",
      "[LightGBM] [Info] Number of positive: 5212, number of negative: 8535\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379137 -> initscore=-0.493212\n",
      "[LightGBM] [Info] Start training from score -0.493212\n",
      "[LightGBM] [Info] Number of positive: 5262, number of negative: 8486\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13748, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382747 -> initscore=-0.477907\n",
      "[LightGBM] [Info] Start training from score -0.477907\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'n_components' parameter of PCA must be an int in the range [0, inf), a float in the range (0.0, 1.0), a str among {'mle'} or None. Got 1.0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_87324/3098346433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Apply PCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Keep specified variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mX_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Initialize variables to track performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpartial_fit_and_fitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m                 \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m             with config_context(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0maccepted\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \"\"\"\n\u001b[0;32m--> 666\u001b[0;31m         validate_parameter_constraints(\n\u001b[0m\u001b[1;32m    667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 )\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             raise InvalidParameterError(\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0;34mf\"The {param_name!r} parameter of {caller_name} must be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;34mf\" {constraints_str}. Got {param_val!r} instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'n_components' parameter of PCA must be an int in the range [0, inf), a float in the range (0.0, 1.0), a str among {'mle'} or None. Got 1.0 instead."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values  # Convert to NumPy array\n",
    "y = data['label'].values  # Convert to NumPy array\n",
    "\n",
    "# Apply PCA to understand variance capture\n",
    "pca = PCA().fit(X)\n",
    "explained_variances = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Range of PCA components to test\n",
    "components_range = np.linspace(0.7, 0.95, 7)  # Testing from 70% to 100% variance\n",
    "avg_test_f1_scores = []\n",
    "\n",
    "# Iterate through different PCA components\n",
    "for n_components in components_range:\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components)  # Keep specified variance\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Initialize variables to track performance\n",
    "    fold_test_f1_scores = []\n",
    "\n",
    "    # Define the k-fold cross-validator\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for train_index, val_index in kf.split(X_pca):\n",
    "        X_train_fold, X_val_fold = X_pca[train_index], X_pca[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "        # Initialize the LightGBM classifier\n",
    "        lgbm = lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            boosting_type='gbdt',\n",
    "            num_leaves=31,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Train the classifier\n",
    "        lgbm.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        y_val_pred = lgbm.predict(X_val_fold)\n",
    "\n",
    "        # Calculate the F1 score on the validation set\n",
    "        fold_test_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "\n",
    "    # Store the average F1 score for the folds\n",
    "    avg_test_f1_scores.append(np.mean(fold_test_f1_scores))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot F1 Score vs. PCA Components\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(components_range, avg_test_f1_scores, marker='o')\n",
    "plt.xlabel('PCA n_components')\n",
    "plt.ylabel('Average Macro F1 Score')\n",
    "plt.title('PCA Components vs. Macro F1 Score')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Explained Variance\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(1, len(explained_variances)+1), explained_variances, marker='o')\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of PCA Components')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (17184, 5000)\n",
      "PCA-transformed shape: (17184, 3440)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the original and transformed data to verify dimensionality\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"PCA-transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4203, number of negative: 6794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.246773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382195 -> initscore=-0.480241\n",
      "[LightGBM] [Info] Start training from score -0.480241\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6792\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382377 -> initscore=-0.479471\n",
      "[LightGBM] [Info] Start training from score -0.479471\n",
      "[LightGBM] [Info] Number of positive: 4248, number of negative: 6750\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.222211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.386252 -> initscore=-0.463094\n",
      "[LightGBM] [Info] Start training from score -0.463094\n",
      "[LightGBM] [Info] Number of positive: 4227, number of negative: 6771\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.234487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384343 -> initscore=-0.471156\n",
      "[LightGBM] [Info] Start training from score -0.471156\n",
      "[LightGBM] [Info] Number of positive: 4185, number of negative: 6813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380524 -> initscore=-0.487326\n",
      "[LightGBM] [Info] Start training from score -0.487326\n",
      "[LightGBM] [Info] Number of positive: 4176, number of negative: 6821\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379740 -> initscore=-0.490652\n",
      "[LightGBM] [Info] Start training from score -0.490652\n",
      "[LightGBM] [Info] Number of positive: 4182, number of negative: 6815\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.278117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380286 -> initscore=-0.488336\n",
      "[LightGBM] [Info] Start training from score -0.488336\n",
      "[LightGBM] [Info] Number of positive: 4166, number of negative: 6832\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.274509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.378796 -> initscore=-0.494661\n",
      "[LightGBM] [Info] Start training from score -0.494661\n",
      "[LightGBM] [Info] Number of positive: 4170, number of negative: 6828\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.271948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379160 -> initscore=-0.493116\n",
      "[LightGBM] [Info] Start training from score -0.493116\n",
      "[LightGBM] [Info] Number of positive: 4190, number of negative: 6808\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380978 -> initscore=-0.485398\n",
      "[LightGBM] [Info] Start training from score -0.485398\n",
      "[LightGBM] [Info] Number of positive: 4199, number of negative: 6798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.277260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381831 -> initscore=-0.481782\n",
      "[LightGBM] [Info] Start training from score -0.481782\n",
      "[LightGBM] [Info] Number of positive: 4132, number of negative: 6865\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.271803 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375739 -> initscore=-0.507674\n",
      "[LightGBM] [Info] Start training from score -0.507674\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.241705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382342 -> initscore=-0.479618\n",
      "[LightGBM] [Info] Start training from score -0.479618\n",
      "[LightGBM] [Info] Number of positive: 4179, number of negative: 6819\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379978 -> initscore=-0.489641\n",
      "[LightGBM] [Info] Start training from score -0.489641\n",
      "[LightGBM] [Info] Number of positive: 4153, number of negative: 6845\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.255592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377614 -> initscore=-0.499687\n",
      "[LightGBM] [Info] Start training from score -0.499687\n",
      "[LightGBM] [Info] Number of positive: 4203, number of negative: 6794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382195 -> initscore=-0.480241\n",
      "[LightGBM] [Info] Start training from score -0.480241\n",
      "[LightGBM] [Info] Number of positive: 4205, number of negative: 6792\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.316420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10997, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382377 -> initscore=-0.479471\n",
      "[LightGBM] [Info] Start training from score -0.479471\n",
      "[LightGBM] [Info] Number of positive: 4248, number of negative: 6750\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.386252 -> initscore=-0.463094\n",
      "[LightGBM] [Info] Start training from score -0.463094\n",
      "[LightGBM] [Info] Number of positive: 4227, number of negative: 6771\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384343 -> initscore=-0.471156\n",
      "[LightGBM] [Info] Start training from score -0.471156\n",
      "[LightGBM] [Info] Number of positive: 4185, number of negative: 6813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.234543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 10998, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.380524 -> initscore=-0.487326\n",
      "[LightGBM] [Info] Start training from score -0.487326\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_87324/3813294015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mlgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Predict on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1266\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    955\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    305\u001b[0m             )\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LGBM_BoosterEvalMethodResultType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4124\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot update due to null objective function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4125\u001b[0m             _safe_call(\n\u001b[0;32m-> 4126\u001b[0;31m                 _LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   4127\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4128\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# List of random seeds and boosting types\n",
    "random_seeds = [42, 52, 62]\n",
    "boosting_types = ['gbdt', 'dart']\n",
    "\n",
    "# Fixed parameters\n",
    "n_estimators = 100\n",
    "num_leaves = 63\n",
    "\n",
    "# Initialize variables to track the best configuration\n",
    "best_f1_score = 0\n",
    "best_params = {}\n",
    "\n",
    "# Store results for all configurations\n",
    "results = []\n",
    "\n",
    "# Iterate over each boosting type\n",
    "for boosting_type in boosting_types:\n",
    "    train_f1_scores = []\n",
    "    test_f1_scores = []\n",
    "\n",
    "    for seed in random_seeds:\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "        # Define the k-fold cross-validator\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "        # Prepare arrays to collect F1 scores\n",
    "        fold_train_f1_scores = []\n",
    "        fold_test_f1_scores = []\n",
    "\n",
    "        # Perform cross-validation\n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            # Initialize the LightGBM classifier\n",
    "            lgbm = lgb.LGBMClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=0.1,\n",
    "                boosting_type=boosting_type,\n",
    "                num_leaves=num_leaves,\n",
    "                random_state=seed\n",
    "            )\n",
    "\n",
    "            # Train the classifier\n",
    "            lgbm.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            # Predict on the validation set\n",
    "            y_val_pred = lgbm.predict(X_val_fold)\n",
    "            y_train_pred = lgbm.predict(X_train_fold)\n",
    "\n",
    "            # Calculate the F1 score on the validation and training sets\n",
    "            fold_train_f1_scores.append(f1_score(y_train_fold, y_train_pred, average='macro'))\n",
    "            fold_test_f1_scores.append(f1_score(y_val_fold, y_val_pred, average='macro'))\n",
    "\n",
    "        # Store the average F1 scores for the folds\n",
    "        train_f1_scores.append(np.mean(fold_train_f1_scores))\n",
    "        test_f1_scores.append(np.mean(fold_test_f1_scores))\n",
    "\n",
    "    # Calculate and store the average F1 scores for this boosting type\n",
    "    average_train_f1_score = np.mean(train_f1_scores)\n",
    "    average_test_f1_score = np.mean(test_f1_scores)\n",
    "    results.append((boosting_type, average_train_f1_score, average_test_f1_score))\n",
    "\n",
    "    # Update best parameters if current average F1 score is better\n",
    "    if average_test_f1_score > best_f1_score:\n",
    "        best_f1_score = average_test_f1_score\n",
    "        best_params = {\n",
    "            'boosting_type': boosting_type\n",
    "        }\n",
    "\n",
    "# Print all results at the end\n",
    "print(\"\\nAll Results:\")\n",
    "for boosting_type, train_f1_score, test_f1_score in results:\n",
    "    print(f\"Boosting Type: {boosting_type}, Average Train F1 Score (Macro): {train_f1_score:.2f}, Average Test F1 Score (Macro): {test_f1_score:.2f}\")\n",
    "\n",
    "# Print the best configuration\n",
    "print(f\"\\nBest F1 Score (Macro): {best_f1_score:.2f}\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (17184, 5000)\n",
      "PCA-transformed shape: (17184, 3440)\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.162524 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 5221, number of negative: 8526\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379792 -> initscore=-0.490431\n",
      "[LightGBM] [Info] Start training from score -0.490431\n",
      "[LightGBM] [Info] Number of positive: 5217, number of negative: 8530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379501 -> initscore=-0.491667\n",
      "[LightGBM] [Info] Start training from score -0.491667\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n",
      "[LightGBM] [Info] Number of positive: 5221, number of negative: 8526\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.286813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379792 -> initscore=-0.490431\n",
      "[LightGBM] [Info] Start training from score -0.490431\n",
      "[LightGBM] [Info] Number of positive: 5217, number of negative: 8530\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.379501 -> initscore=-0.491667\n",
      "[LightGBM] [Info] Start training from score -0.491667\n",
      "[LightGBM] [Info] Number of positive: 5267, number of negative: 8480\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 877200\n",
      "[LightGBM] [Info] Number of data points in the train set: 13747, number of used features: 3440\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383138 -> initscore=-0.476250\n",
      "[LightGBM] [Info] Start training from score -0.476250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1p/dxp_xdmj2g1bm4xq3hqqrnx40000gn/T/ipykernel_87324/3651115620.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mlgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Predict on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1266\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    955\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    305\u001b[0m             )\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LGBM_BoosterEvalMethodResultType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4124\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot update due to null objective function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4125\u001b[0m             _safe_call(\n\u001b[0;32m-> 4126\u001b[0;31m                 _LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   4127\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4128\u001b[0m                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('data/train_tfidf_features.csv')\n",
    "\n",
    "# Exclude the first column and use the second column as the label\n",
    "X = data.drop(['label', 'id'], axis=1).values  # Convert to NumPy array\n",
    "y = data['label'].values  # Convert to NumPy array\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of the variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Print the shapes of the original and transformed data to verify dimensionality\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"PCA-transformed shape:\", X_pca.shape)\n",
    "\n",
    "# List of random seeds and colsample_bytree values to test\n",
    "random_seeds = [42, 52, 62]\n",
    "colsample_bytree_values = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Fixed parameters\n",
    "n_estimators = 100\n",
    "num_leaves = 63\n",
    "boosting_type = 'gbdt'\n",
    "\n",
    "# Initialize variables to track the best configuration\n",
    "best_f1_score = 0\n",
    "best_params = {}\n",
    "\n",
    "# Store results for all configurations\n",
    "results = []\n",
    "\n",
    "# Iterate over each colsample_bytree value\n",
    "for colsample_bytree in colsample_bytree_values:\n",
    "    test_f1_scores = []\n",
    "\n",
    "    for seed in random_seeds:\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "        # Initialize the LightGBM classifier\n",
    "        lgbm = lgb.LGBMClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=0.1,\n",
    "            boosting_type=boosting_type,\n",
    "            num_leaves=num_leaves,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            random_state=seed\n",
    "        )\n",
    "\n",
    "        # Train the classifier\n",
    "        lgbm.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_test_pred = lgbm.predict(X_test)\n",
    "\n",
    "        # Calculate the F1 score on the test set\n",
    "        test_f1_score = f1_score(y_test, y_test_pred, average='macro')\n",
    "        test_f1_scores.append(test_f1_score)\n",
    "\n",
    "    # Calculate and store the average F1 score for this colsample_bytree value\n",
    "    average_f1_score = np.mean(test_f1_scores)\n",
    "    results.append((colsample_bytree, average_f1_score))\n",
    "\n",
    "    # Update best parameters if current average F1 score is better\n",
    "    if average_f1_score > best_f1_score:\n",
    "        best_f1_score = average_f1_score\n",
    "        best_params = {\n",
    "            'colsample_bytree': colsample_bytree\n",
    "        }\n",
    "\n",
    "# Print all results at the end\n",
    "print(\"\\nAll Results:\")\n",
    "for colsample_bytree, f1_score in results:\n",
    "    print(f\"colsample_bytree: {colsample_bytree}, Average F1 Score (Macro): {f1_score:.2f}\")\n",
    "\n",
    "# Print the best configuration\n",
    "print(f\"\\nBest F1 Score (Macro): {best_f1_score:.2f}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Plot the results\n",
    "colsample_bytree_values, average_f1_scores = zip(*results)\n",
    "plt.plot(colsample_bytree_values, average_f1_scores, marker='o')\n",
    "plt.xlabel('colsample_bytree')\n",
    "plt.ylabel('Average F1 Score (Macro)')\n",
    "plt.title('Effect of colsample_bytree on F1 Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
